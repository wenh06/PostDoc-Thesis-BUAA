\section{联邦学习的起源与演化}
\addcontentsline{toe}{section}{{\thecurrentchapter .2\ \ Origin and Development of Federal Learning}\numberline\,}
\label{sec:chap1-fl-origin}

% almost finished
% indexed

联邦学习Federated Learning\index{联邦学习, Federated Learning}这个名词首次由Google的研究人员McMahan等人于2016年在文章\emph{Communication-Efficient Learning of Deep Networks from Decentralized Data}\cite{mcmahan2017fed_avg}中提出：
\begin{quote}
    ``We term our approach Federated Learning, since the learning task is solved by a loose federation of participating devices (which we refer to as clients) which are coordinated by a central server.''
\end{quote}
文章\parencite{mcmahan2017fed_avg}最初目的，是希望能利用用户手机端的数据改进安卓手机上输入法的预测，在防止数据泄露的前提下，基于分布在多个设备上的数据集构建机器学习模型\cite{hard2018_fl_keyboard,yang2018_fl_google}。具有里程碑意义的综述文章\emph{Advances and Open Problems in Federated Learning}\cite{kairouz2019advances_fl} 给联邦学习下过如下的定义
\begin{quote}
    ``Federated learning is a machine learning setting where multiple entities (clients) collaborate in solving a machine learning problem, under the coordination of a central server or service provider. Each client's raw data is stored locally and not exchanged or transferred; instead, focused updates intended for immediate aggregation are used to achieve the learning objective.''
\end{quote}
也就是说，联邦学习是一种新的机器学习范式，适用于有中心节点 (Server) \index{中心节点, Server}居中协调，多个子节点 (Client) \index{子节点, Client}进行协作参与，进行联合建模的场景。在这种场景下，联邦学习的方法能够在每个参与方不暴露己方拥有的数据的情况下，完成联合建模的任务。一般来说，在联邦学习的框架下，数据拥有方在不用给出己方数据的情况下，也可进行模型训练得到公共的模型$M_{\text{fed}}$，使得模型$M_{text{fed}}$，与将数据集中到一起进行训练能得到的模型$M$，二者的预测值的偏差的期望能足够小。
\begin{equation}
\label{eq:general-fl}
\expectation_{z\sim\mathcal{D}} \lVert M_{text{fed}}(z) - M(z) \rVert \leqslant \delta.
\end{equation}
其中$\mathcal{D}$是总体训练数据的分布，$\delta$是某个我们能够接受的偏差的上界。尽管分布式优化 (Distributed Optimization) \index{分布式优化, Distributed Optimization}、加密计算等问题在联邦学习这一概念被提出之前就有学者进行了研究\cite{boyd2011distributed, dist_pca_2014_nips, Gentry_2009_FHE, Nikolaenko_2013}, 但是直到联邦学习被正式提出\cite{mcmahan2017fed_avg}，以上关于联邦学习独有的一些特性以及需求才真正被学界以及工业界所重视，相关的研究才开始蓬勃发展。需要注意的是，尽管文献\parencite{kairouz2019advances_fl}给出的联邦学习的定义强调了有中心节点这一特征，同时大部分关于联邦学习的文献也是基于这一假设开展研究的，但仍有一部分关于联邦学习的文献是基于无中心的完全分布式的节点网络进行研究的\cite{elgabli2020gadmm, issaid2020cq-ggadmm}.

从上面关于联邦学习的定义中我们可以看出，一般来说，联邦学习的参与方构成了一个有中心的网络。我们在这里先明确关于这个网络的一些基本的术语：
\begin{itemize}
    \item Server: 中心节点，负责整个联邦学习网络的控制、编排子节点任务的工作，同时维护全局共享的模型，其本身一般没有训练数据；
    \item Client: 子节点，拥有一定数量的训练数据，利用这些数据以及中心节点共享的信息，执行本地模型训练的任务。
\end{itemize}

我国杨强教授领导的香港科技大学以及微众银行 (WeBank) 的AI研究团队，从经济金融行业实践出发，将文献\parencite{mcmahan2017fed_avg}考虑的联邦学习经典的应用场景,即超大规模的移动、边缘设备协同参与模型训练的场景，扩展到了多个数据不互通的数据中心的场景\cite{Yang_2019_VFL}。前者被抽象为了\textbf{跨设备} (Cross-Device) \index{跨设备, Cross-Device}的场景，后者被抽象为了\textbf{跨(数据)仓库} (Cross-Silo) \index{跨(数据)仓库, Cross-Silo}的场景\cite{kairouz2019advances_fl}。跨设备的场景具有子节点数量多，每个子节点持有数据量少，算力低，通信带宽低且不稳定，对通信开销非常敏感等特点；跨仓库的场景具有子节点数量少 (数量级通常在$10^2$以内)，每个子节点持有数据量大，算力高，对数据的安全性、保密性要求高等特点，子节点与中心节点的通信开销与子节点本身的算力都可能成为瓶颈。这两种经过高度抽象的场景基本涵盖了联邦学习面临的所有的场景。文献\parencite{kairouz2019advances_fl}的Table 1对于这两种场景的联邦学习的特点进行了全面的总结与对比。

文献\parencite{Yang_2019_VFL}的另一个突出的贡献是从数据分布的角度，具体来说，是样本特征空间 (Sample Feature Space) \index{样本特征空间, Sample Feature Space}以及样本编号空间 (Sample ID Space) \index{样本编号空间, Sample ID Space}的角度，将联邦学习分为三类：横向联邦学习 (Horizontal Federated Learning, HFL) \index{横向联邦学习, Horizontal Federated Learning, HFL}、纵向联邦学习 (Vertical Federated Learning, VFL) \index{纵向联邦学习, Vertical Federated Learning, VFL}和联邦迁移学习 (Federated Transfer Learning, FTL) \index{联邦迁移学习, Federated Transfer Learning, FTL}。

横向联邦学习是最常见的一类联邦学习，即每个参与方数据的特征空间基本是一致的，但各自拥有的样本编号基本是不重叠的。一般情况下，我们谈论的联邦学习，以及跨设备的联邦学习，都是横向联邦学习。纵向联邦学习则正好与横向联邦学习相反，纵向联邦学习的每个参与方有重叠度较高的样本编号空间，但是参与方之间拥有的每个样本的特征的重叠度很低。这在金融风控等领域是非常常见的。联邦迁移学习则是迁移学习 (Transfer Learning) \index{迁移学习, Transfer Learning}这一概念在联邦学习这一范畴内的具体应用，首先于文献\parencite{liu_2020_transfer_fl}中被系统地研究。联邦迁移学习的核心思想是，源领域 (Source Domain) 和目标领域 (Target Domain) 之间的相似性，将机器学习模型从源领域学习到的知识迁移应用到目标领域。源领域往往是知识丰富的领域 (具有丰富的标签)，而目标领域则是知识欠丰富，需要接受知识迁移的领域。联邦迁移学习天然适合于智慧医疗领域，例如不同医疗机构的医学影像数据一般都是不同的，但基础都是影像特征的提取与利用。

一般来说，假设一个联邦学习系统有$K$个参与方，每个参与方拥有的数据分布记作$\mathcal{D}_k,$ $k = 1, \ldots, K.$ $\mathcal{D}_k$有分解
\begin{equation*}
\mathcal{D}_k \subseteq \mathcal{X}_k \times \mathcal{Y}_k \times \mathcal{I}_k,
\end{equation*}
其中$\mathcal{X}_k$为数据样本的特征空间，$\mathcal{Y}_k$为数据样本的标签空间 (Sample Label Space) \index{样本标签空间, Sample Label Space}，$\mathcal{I}_k$为数据样本编号空间。那么有\cite{Yang_2019_VFL,vfl}
\begin{itemize}
\item 横向联邦学习:
\begin{equation*}
\mathcal{X}_i \eqabove{d} \mathcal{X}_j, ~ \mathcal{Y}_i \eqabove{d} \mathcal{Y}_j, ~ \mathcal{I}_i \neqabove{d} \mathcal{I}_j, ~ \forall i \neq j;
\end{equation*}
以上的符号$\eqabove{d}$表示相应的概率空间是同分布的。横向联邦学习一般可以简化建模为如下的最优化问题\cite{kairouz2019advances_fl}
\begin{equation}
\label{eq:general-hfl}
\begin{array}{cl}
\text{minimize} & f(\theta) = \expectation\limits_{k \sim {\mathcal{P}}} [f_k(\theta)] = \sum\limits_{k=1}^K w_k f_k(\theta), \\
\text{where} & f_k(\theta) = \expectation\limits_{(x, y) \sim \mathcal{D}_k} [\ell_k(\theta; x, y)],
\end{array}
\end{equation}
其中符号$\expectation$表示关于分布的期望，$\theta$是模型参数，也就是我们的学习目标；$\mathcal{P}$是参与训练的节点的分布，这里我们假设了总共有$K$个节点；$w_k$是节点$k$的权重，一般与节点持有的数据量正相关；$\ell_k$是节点$k$上模型训练的损失函数 (Loss Function) \index{损失函数, Loss Function}或者目标函数 (Objective Function) \index{目标函数, Objective Function}；$x, y$分别表示数据的特征与数据的标签。$f_k$以及$f$则被称作局部目标函数以及全局目标函数。这里，我们忽略了数据样本编号空间，将数据分布简写为了$(x, y) \sim \mathcal{D}_k \subseteq \mathcal{X}_k \times \mathcal{Y}_k.$
\item 纵向联邦学习:
\begin{equation*}
\mathcal{X}_i \neqabove{d} \mathcal{X}_j, ~ \mathcal{Y}_i \neqabove{d} \mathcal{Y}_j, ~ \mathcal{I}_i \eqabove{d} \mathcal{I}_j, ~ \forall i \neq j;
\end{equation*}
纵向联邦学习对应的最优化模型可以写为\cite{vfl}
\begin{equation}
\label{eq:general-vfl}
\begin{array}{cl}
\text{minimize} & f(\theta) = \sum\limits_{i=1}^N \mathcal{L} \left( \mathcal{F}_K \left( \psi_K; f_{1}(\theta_{1}; x_{i}^{(1)}), \ldots, f_{K}(\theta_{K}; x_{i}^{(K)}), y_{i} \right) \right), \\
\text{where} & \theta = (\theta_{1}, \ldots, \theta_{K}), ~ x_{i} = (x_{i}^{(1)}, \ldots, x_{i}^{(K)}),
\end{array}
\end{equation}
其中$N$是总样本数，也就是说我们有$N$条数据；$x_{i}^{(k)}$为参与联邦学习的节点$k$掌握的样本$i$的部分特征，所有节点掌握的样本$i$的特征按一定规则排列构成了样本$i$的总体特征$x_{i} = (x_{i}^{(1)}, \ldots, x_{i}^{(K)}).$ 样本$i$的标签一般都假设在某一个(例如节点$K$)参与纵向联邦学习训练的节点上，这个节点被称作积极节点 (Active Party)，其余节点被称作被动节点 (Passive Parties)。 除了标签$y,$ 积极节点$K$上还拥有的直接与训练任务相关的所谓的全局模块 (Global Module) $\mathcal{F}_K,$ 其参数为$\psi_K.$ 全局模块可能是简单的求平均的模块，也可以是一个神经网络的最顶层的用于最终进行回归或者分类任务的全连接层。$\mathcal{L}$是训练任务的损失函数。
\item 联邦迁移学习:
\begin{equation*}
\mathcal{X}_i \neqabove{d} \mathcal{X}_j, ~ \mathcal{Y}_i \neqabove{d} \mathcal{Y}_j, ~ \mathcal{I}_i \neqabove{d} \mathcal{I}_j, ~ \forall i \neq j;
\end{equation*}
一个有$A, B$两方参与的联邦迁移学习可以简化为如下的模型\cite{liu_2020_transfer_fl}
\begin{equation}
\label{eq:general-ftl}
\text{minimize} \quad \sum_{i=1}^{N_{AB}} \left( \ell_1(y_i^A, \varphi(u_i^B)) + \lambda \ell_2(u_i^A, u_i^B)  \right),
\end{equation}
其中$A$是源领域，$B$是目标领域；$N_{AB}$是$A, B$两方共有的样本数量；$u_i^A, u_i^B \in \mathcal{U},$ $\mathcal{U}$是$A, B$两方共有的隐式特征表示空间(Hidden Representation Space)，$A, B$两方的特征通过如下的映射(例如通过两个神经网络)映到同一个空间$\mathcal{U}$
\begin{gather*}
\mathcal{X}^A \to \mathcal{U}: ~ x_i^A \mapsto u_i^A, \\ \mathcal{X}^B \to \mathcal{U}: ~ x_i^B \mapsto u_i^B.
\end{gather*}
$\ell_1$是训练任务的损失函数，$\ell_2$是所谓的对齐损失函数 (Alignment Loss Function)，可以取
\begin{equation*}
- \langle u_i^A, u_i^B \rangle, ~~ \text{或者} ~ \lVert u_i^A, u_i^B \rVert^2.
\end{equation*}
$\varphi$是一个所谓的翻译函数 (Translator Function)，用于对$u_i^B$进行打标签的，一般也可以利用隐式表示特征的相关性取作
\begin{equation}
\label{eq:ftl-translator-func}
\varphi(u_i^B) = \frac{1}{N_A} \sum\limits_{i=1}^{N_A} y_i^A \langle u_i^A, u_i^B \rangle,
\end{equation}
其中$N_{A}$是源领域$A$拥有的样本数量。
\end{itemize}
我们可以用图\ref{fig:three-types-fl}~来概括地表示这三类联邦学习在以上几种概率空间上的特点。

\input{tikz-figures/chap1-3types-fl.tex}

一般来说，一个完整的联邦学习的迭代流程可以归纳如下
\begin{itemize}
    \item 子节点选取 (Client Selection)；
    \item 中心节点将子节点计算需要的信息(例如模型参数、梯度等)下发到被选取的子节点 (Parameter Broadcast)；
    \item {\bfseries 子节点执行本地计算，更新本地模型参数 (Local Parameter Update);}
    \item 子节点将更新后的信息返回中心节点，中心节点将收集到的信息进行聚合 (Parameter Aggregation);
    \item {\bfseries 中心节点执行计算，更新全局模型 (Global Parameter Update)。}
\end{itemize}
可以看到，这一流程关于模型参数的更新完全是解耦合的，因此子节点模型更新方法与中心节点的全局模型的更新方法可以做相对灵活的组合，只要中心节点与子节点之间定义好了传输信息的协议。我们可以把以上的流程用图\ref{fig:broadcast-local-update}~与图\ref{fig:agg-global-update}~来进行简单的示意。

\input{tikz-figures/chap1-fl-process.tex}
