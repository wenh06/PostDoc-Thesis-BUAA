\section{联邦学习中的原始对偶算法}
\addcontentsline{toe}{section}{{2.4\ \ Primal-Dual Algorithms in Federated Learning}\numberline\,}
\label{sec:chap2-primal-dual}

% NOT finished

在传统的最优化方法中，原始-对偶算法(Primal-Dual Algorithms)也是一类常用的算法。我们考虑格式为~\eqref{eq:fl-basic-constraint}~的带等式线性约束的共识优化问题(注意与~\eqref{eq:fl-basic-constraint}~的细微差别)
\begin{equation}
\label{eq:fedpd-constraint}
\begin{array}{cl}
\minimize & F(\Theta) := \sum\limits_{k=1}^K f_k(\theta_k), \\
\text{subject to} & \theta_k = \theta, ~ k = 1, \ldots, K.
\end{array}
\end{equation}
其中$\Theta = \col(\theta_1, \cdots, \theta_K), ~ \theta, \theta_1, \ldots, \theta_K \in \R^d.$ 约束优化问题~\eqref{eq:fedpd-constraint}~的增广拉格朗日函数(Augmented Lagrangian, AL)为
\begin{equation}
\label{eq:al}
\mathcal{L}(\Theta, \Lambda) = F(\Theta) - \sum\limits_{k=1}^K \left\{ \langle \lambda_k, \theta_k \rangle + \frac{1}{2\eta} \lVert \theta_k - \theta \rVert^2 \right\} = \sum\limits_{k=1}^K \mathcal{L}_k(\theta, \theta_k, \lambda_k)
\end{equation}
其中
\begin{equation}
\label{eq:al-local}
\mathcal{L}_k(\theta, \theta_k, \lambda_k) = f_k(\theta_k) + \langle \lambda_k, \theta_k - \theta \rangle + \frac{1}{2\eta} \lVert \theta_k - \theta \rVert^2,
\end{equation}
$\Lambda = \col(\lambda_1, \ldots, \lambda_K)$被称作对偶变量(Dual Variable)或者拉格朗日乘子(Lagrangian Multiplier)。

待写

文献\parencite{zhang2020fedpd}研究了

\input{algorithms/fedpd}

待写。。。。
