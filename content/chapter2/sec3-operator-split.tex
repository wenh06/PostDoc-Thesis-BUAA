\section{联邦学习中的算子分裂算法}
\addcontentsline{toe}{section}{{2.3\ \ Operator Splitting Algorithms in Federated Learning}\numberline\,}
\label{sec:chap2-operator-split}

我们已经讨论过的通用加速技巧\cite{reddi2020fed_opt}，方差缩减技术\cite{karimireddy2020scaffold}，以及临近点方法\cite{sahu2018fedprox}，解决的都是联邦学习在各种各样场景下的收敛性、收敛率相关的问题，而算法收敛结果的正确性这一问题始终没有得到重视。我们来举一个非常简单的例子来说明这个问题。

\begin{example}
\label{eg:correctness}
假设我们要拟合线性模型$y = mx$, $m$为需要拟合的变量，使用均方误差(Mean Squared Error, MSE)做损失函数 (目标函数)。我们有两个参与方，参与方1的拥有的训练数据为$\{ (0, 2), (1, 2) \},$ 相应的目标函数为
\begin{equation*}
f_1(m_1) = 4 + (m_1 - 2)^2;
\end{equation*}
参与方2的拥有的训练数据为$\{ (2, 0), (2, 1) \},$ 相应的目标函数为
\begin{equation*}
f_2(m_2) = 4m_2^2 + (2m_2 - 1)^2.
\end{equation*}
那么参与方1单独拟合结果为$m_1 = 2$, 参与方2单独拟合结果为$m_2 = \frac{1}{4}$, 采用联邦平均算法\texttt{FedAvg}的结果是$m = \frac{9}{8}.$ 而采用联邦临近点算法\texttt{FedProx}的结果(计算依据见定理\ref{thm:fedsplit-correctness})是
\begin{equation}
\label{eq:eg-correctness-fedprox}
m = \frac{36 + 4\mu}{32 + 9\mu},
\end{equation}
其中$\mu$为临近项相关的系数 (见式\eqref{eq:fedprox})。

数据都拿一起拟合结果为$k = \frac{4}{9}.$ 注意，这里一起拟合的结果并不等于单独拟合的均值 (\texttt{FedAvg}的结果)，原因就在于loss function里的k是2次的，不是线性的。
\end{example}

文献\cite{pathak2020fedsplit}首先注意到这个问题，并具体分析了联邦平均算法\texttt{FedAvg}与联邦临近点算法\texttt{FedProx}的理论收敛结果，如下

\begin{theorem}
\label{thm:fedsplit-correctness}
假设我们有$K$个节点参与联邦学习，每一个节点$k \in [K]$的目标函数为$f_k(\theta_k),$ 那么
\begin{itemize}
\item[(1)] 若采用联邦平均算法\texttt{FedAvg} \ref{algo:fedavg}，且假设子节点上的参数更新方式为单步 (即$R = 1$) 的梯度下降。记梯度映射为
\begin{equation*}
\mathcal{G}_k (\theta_k) := \theta_k - \eta \nabla f_k (\theta_k),
\end{equation*}
并记$\mathcal{G}^e_k (\theta_k) := \underbrace{G_k\circ\cdots\circ G_k}_{e-\text{次复合}} (\theta_k).$ 若算法 \ref{algo:fedavg} 生成的全局模型参数的序列$\{ \theta^{(t)} \}_{t=1}^{\infty}$收敛，那么所有子节点$k \in [K]$模型参数的序列$\{ \theta_k^{(t)} \}_{t=1}^{\infty}$有公共的极限$\theta^*,$ 且$\theta^*$满足下列不动点条件
\begin{equation}
\label{eq:fedgd-fixed-pt}
\sum\limits_{i=1}^e \sum\limits_{k=1}^K \nabla f_k(\mathcal{G}_k^{i-1}(\theta^*)) = 0.
\end{equation}
\item[(2)] 若采用联邦临近点算法\texttt{FedProx} \ref{algo:fedprox}，记Moreau包络映射\eqref{eq:moreau_env}为
\begin{equation}
\label{eq:moreau-mapping}
\mathcal{M}_{\mu, f_k} (\theta) := \inf\limits_{\theta_k} \left\{ f_k(\theta_k) + \frac{\mu}{2} \lVert \theta - \theta_k \rVert^2 \right\}.
\end{equation}
若算法 \ref{algo:fedprox} 生成的全局模型参数的序列$\{ \theta^{(t)} \}_{t=1}^{\infty}$收敛，那么所有子节点$k \in [K]$模型参数的序列$\{ \theta_k^{(t)} \}_{t=1}^{\infty}$有公共的极限$\theta^*,$ 且$\theta^*$满足下列不动点条件
\begin{equation}
\label{eq:fedprox-fixed-pt}
\sum\limits_{k=1}^K \nabla \mathcal{M}_{\mu, f_k} (\theta^*) = 0.
\end{equation}
\end{itemize}
\end{theorem}

待写。。。

\input{algorithms/fedsplit.tex}


我们从最优化理论的角度对FedSplit算法进行分析。事实上，我们考虑的优化问题\eqref{eq:fl-basic}可以等价地改写为如下的共识优化问题(注意与式\eqref{eq:fl-basic-constraint}的细微差别)
\begin{equation}
\label{eq:fedsplit-constraint}
\begin{array}{cl}
\minimize & F(\Theta) := \sum\limits_{k=1}^K w_k f_k(\theta_k), \\
\text{subject to} & A \Theta = 0,
\end{array}
\end{equation}
其中
\begin{equation*}
A = \begin{pmatrix} I_d & -I_d & & & \\ & I_d & -I_d & & \\ & & \ddots & \ddots & \\ & & & \ddots & -I_d \\ -I_d & & & & I_d \end{pmatrix}, ~ \Theta = \begin{pmatrix} \theta_1 \\ \vdots \\ \theta_K \end{pmatrix}, ~ \theta_1, \ldots, \theta_K \in \R^d.
\end{equation*}
约束优化问题\eqref{eq:fedsplit-constraint}的拉格朗日函数为
\begin{equation}
\label{eq:fedsplit-lagrangian}
\mathcal{L}(\Theta, \Lambda) = F(\Theta) - \langle \Lambda, A\Theta \rangle = \sum\limits_{k=1}^K w_k f_k(\theta_k) - \langle \Lambda, A\Theta \rangle,
\end{equation}
其中$\Lambda = \begin{pmatrix} \lambda_1 \\ \vdots \\ \lambda_K \end{pmatrix} \in \R^{Kd}$为对偶变量。考虑上述问题的一阶最优性条件$\nabla F(\Theta) - A^T \Lambda,$ 即
\begin{equation*}
\nabla F(\Theta) - \begin{pmatrix} \lambda_1 - \lambda_K \\ \lambda_2 - \lambda_1 \\ \vdots \\ \lambda_K - \lambda_{K-1} \end{pmatrix} = 0.
\end{equation*}
上式是一个所谓的单调包含问题(Monotone Inclusion Problem)
\begin{equation}
\label{eq:fedsplit-mono-incl}
0 \in \nabla F(\Theta) + \mathcal{N}_{\mathcal{E}}(\Theta)
\end{equation}
其中
\begin{equation*}
\mathcal{N}_{\mathcal{E}}(\Theta) = \begin{cases} \mathcal{E}^{\perp} & \text{ if } \Theta \in \mathcal{E} \\ \emptyset & \text{ otherwise } \end{cases}
\end{equation*}
是凸集$\mathcal{E} = \{ \Theta \in \R^{Kd} ~ | ~ \theta_1 = \cdots = \theta_K\}$在点$\Theta$处的法锥(Normal Cone)。

我们回忆一下法锥(Normal Cone)的定义：
\begin{definition}[法锥(Normal Cone)]
设$\Omega \in \R^n$是$n$维欧氏空间中的非空凸集，$x \in \Omega$是其中一点，那么非空凸集$\Omega$在点$x$处的法锥(Normal Cone)为
\begin{equation}
\label{eq:def-normal-cone}
\mathcal{N}_{\Omega}(x) = \left\{ v ~ \middle| ~ \langle v, w - x \rangle \leqslant 0 ~ \forall w \in \Omega \right\}.
\end{equation}
\end{definition}


\input{algorithms/feddr.tex}
