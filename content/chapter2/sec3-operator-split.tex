\section{联邦学习中的算子分裂算法}
\addcontentsline{toe}{section}{{2.3\ \ Operator Splitting Algorithms in Federated Learning}\numberline\,}
\label{sec:chap2-operator-split}

我们已经讨论过的通用加速技巧\cite{reddi2020fed_opt}，方差缩减技术\cite{karimireddy2020scaffold}，以及临近点方法\cite{sahu2018fedprox}，解决的都是联邦学习在各种各样场景下的收敛性、收敛率相关的问题，而算法收敛结果的正确性这一问题始终没有得到重视。我们来举一个非常简单的例子来说明这个问题。

\begin{example}
\label{eg:correctness}
假设我们要拟合线性模型$y = mx$, $m$为需要拟合的变量，使用均方误差(Mean Squared Error, MSE)做损失函数 (目标函数)，即我们考虑的是一个最小二乘问题。假设我们有两个参与方，参与方1的拥有的训练数据为$\{ (0, 2), (1, 2) \},$ 相应的目标函数为
\begin{equation*}
f_1(m_1) = \frac{1}{2} (4 + (m_1 - 2)^2);
\end{equation*}
参与方2的拥有的训练数据为$\{ (2, 0), (2, 1) \},$ 相应的目标函数为
\begin{equation*}
f_2(m_2) = \frac{1}{2} (4m_2^2 + (2m_2 - 1)^2).
\end{equation*}
那么参与方1单独拟合结果为$m_1 = 2$, 参与方2单独拟合结果为$m_2 = \frac{1}{4}$, 采用联邦平均算法\texttt{FedAvg}的结果是$m = \frac{9}{8}.$ 而采用联邦临近点算法\texttt{FedProx}的结果(计算方法见定理\ref{thm:fedsplit-correctness})是
\begin{equation}
\label{eq:eg-correctness-fedprox}
m = \frac{18 + 4\mu}{16 + 9\mu},
\end{equation}
其中$\mu$为临近项相关的系数 (见式~\eqref{eq:fedprox})。

数据都拿一起拟合结果为$k = \frac{4}{9}.$ 注意，这里一起拟合的结果并不等于单独拟合的均值 (\texttt{FedAvg}的结果)，原因就在于loss function里的k是2次的，不是线性的。
\end{example}

文献\cite{pathak2020fedsplit}首先注意到这个问题，并具体分析了联邦平均算法\texttt{FedAvg}与联邦临近点算法\texttt{FedProx}的理论收敛结果，如下

\begin{theorem}
\label{thm:fedsplit-correctness}
假设我们有$K$个节点参与联邦学习，每一个节点$k \in [K]$的目标函数为$f_k(\theta_k),$ 这些函数都是有限凸函数，而且都是$L$-光滑的(定义见式~\eqref{eq:l-smooth})，那么
\begin{itemize}
\item[(1)] 若采用联邦平均算法\texttt{FedAvg} \ref{algo:fedavg}，且假设子节点上的参数更新方式为全量的梯度下降。我们把这种简化的算法记为\texttt{FedGD}，记梯度映射为
\begin{equation}
\label{eq:grad-mapping}
\mathcal{G}_k (\theta_k) := \theta_k - \eta \nabla f_k (\theta_k),
\end{equation}
并记$\mathcal{G}^i_k (\theta_k) := \underbrace{G_k\circ\cdots\circ G_k}_{i-\text{次复合}} (\theta_k).$ 若算法 \ref{algo:fedavg}~生成的全局模型参数的序列$\{ \theta^{(t)} \}_{t=1}^{\infty}$收敛，那么所有子节点$k \in [K]$模型参数的序列$\{ \theta_k^{(t)} \}_{t=1}^{\infty}$有公共的极限$\theta^*,$ 且$\theta^*$满足下列不动点条件
\begin{equation}
\label{eq:fedgd-fixed-pt}
\sum\limits_{i=1}^R \sum\limits_{k=1}^K \nabla f_k(\mathcal{G}_k^{i-1}(\theta^*)) = 0.
\end{equation}
\item[(2)] 若采用联邦临近点算法\texttt{FedProx} \ref{algo:fedprox}，记Moreau包络映射~\eqref{eq:moreau_env}~为
\begin{equation}
\label{eq:moreau-mapping}
\mathcal{M}_{f_k, \mu} (\theta) := \inf\limits_{\theta_k} \left\{ f_k(\theta_k) + \frac{\mu}{2} \lVert \theta - \theta_k \rVert^2 \right\}.
\end{equation}
若算法 \ref{algo:fedprox}~生成的全局模型参数的序列$\{ \theta^{(t)} \}_{t=1}^{\infty}$收敛，那么所有子节点$k \in [K]$模型参数的序列$\{ \theta_k^{(t)} \}_{t=1}^{\infty}$有公共的极限$\theta^*,$ 且$\theta^*$满足下列不动点条件
\begin{equation}
\label{eq:fedprox-fixed-pt}
\sum\limits_{k=1}^K \nabla \mathcal{M}_{f_k, \mu} (\theta^*) = 0.
\end{equation}
\end{itemize}
\end{theorem}

\begin{proof}
\begin{itemize}
\item[(1)] 记$\Theta^{(t)} = (\theta_1^{(t)}, \cdots, \theta_K^{(t)}).$ 由于算法 \ref{algo:fedavg}~生成的全局模型参数的序列$\{ \theta^{(t)} \}_{t=1}^{\infty}$收敛，也就是说约束优化问题~\eqref{eq:fedavg-constraint}~有解$\Theta^* = (\theta_1^*, \cdots, \theta_K^*).$ 这个解要满足问题~\eqref{eq:fedavg-constraint}~的约束条件，即满足
\begin{equation*}
\theta_1^* = \cdots = \theta_K^*,
\end{equation*}
那么我们就知道了所有子节点$k \in [K]$模型参数的序列$\{ \theta_k^{(t)} \}_{t=1}^{\infty}$有公共的极限$\theta^* = \theta_1^* = \cdots = \theta_K^*.$ 全局模型参数序列
\begin{equation*}
\theta^{(t)} = \frac{1}{K} (\theta_1^{(t)} + \cdots + \theta_K^{(t)}), ~~ t = 1, \ldots
\end{equation*}
也收敛于$\theta^*,$ 从而有
\begin{equation*}
\theta^* = \texttt{FedGD} (\theta^*) := \frac{1}{K} ( \mathcal{G}^R_1 (\theta^*) + \cdots + \mathcal{G}^R_K (\theta^*) )
\end{equation*}
即
\begin{equation*}
0 = \theta^* - \frac{1}{K} \sum\limits_{k=1}^K \mathcal{G}^R_k (\theta^*)
\end{equation*}
我们把梯度映射~\eqref{eq:grad-mapping}~的定义反复代入上式中，即有
\begin{align*}
0 = & \frac{1}{K}\sum_{k=1}^K \mathcal{G}^R_k(\theta^{*}) - \theta^{*} = \frac{1}{K} \sum_{k=1}^K \mathcal{G}_k ( \mathcal{G}^{R-1}_k (\theta^{*}) ) - \theta^{*} \\
= & \frac{1}{K}\sum_{k=1}^K \left( \mathcal{G}^{R-1}_k(\theta^{*}) - \eta\nabla f_k (\mathcal{G}^{R-1}_k(\theta^{*})) \right) - \theta^{*} \\
= & \frac{1}{K}\sum_{k=1}^K \mathcal{G}^{R-1}_k(\theta^{*}) - \theta^{*} - \frac{\eta}{K} \sum_{k=1}^K \nabla f_k (\mathcal{G}^{R-1}_k(\theta^{*})) \\
& \hspace{3em} \vdots \\
= & \frac{1}{K}\sum_{k=1}^K \mathcal{G}^{0}_k(\theta^{*}) - \theta^{*} - \frac{\eta}{K} \sum\limits_{i=1}^R \sum\limits_{k=1}^K \nabla f_k( \mathcal{G}_k^{i-1}(\theta^*) ) \\
= & - \frac{\eta}{K} \sum\limits_{i=1}^R \sum\limits_{k=1}^K \nabla f_k( \mathcal{G}_k^{i-1}(\theta^*) ).
\end{align*}
这样，我们就证明了$\theta^*$满足不动点条件~\eqref{eq:fedgd-fixed-pt}.
\item[(2)] 关于联邦临近点算法\texttt{FedProx}~\ref{algo:fedprox}~在所有子节点上产生的模型参数序列有公共极限$\theta^*$的论断同(1)可证，我们来证明这个极限满足不动点条件~\eqref{eq:fedprox-fixed-pt}。由于$f_k$是光滑凸函数，因此有
\begin{equation*}
\prox_{f_k, \mu} (\theta) = \theta - \frac{1}{\mu} \nabla \mathcal{M}_{f_k, \mu}(\theta),
\end{equation*}
于是我们有
\begin{align*}
0 & = \theta^* - \frac{1}{K}\sum_{k=1}^K \prox_{f_k, \mu}(\theta^*) \\
& = \theta^* - \frac{1}{K} \sum_{k=1}^K \left( \theta^* - \frac{1}{\mu} \nabla \mathcal{M}_{f_k, \mu}(\theta^*) \right) \\
& = \theta^* - \frac{1}{K} \sum_{k=1}^K \theta^* + \frac{1}{\mu K} \sum_{k=1}^K \nabla \mathcal{M}_{f_k, \mu}(\theta^*) \\
& = \frac{1}{\mu K} \sum_{k=1}^K \nabla \mathcal{M}_{f_k, \mu}(\theta^*).
\end{align*}
\end{itemize}
\end{proof}

\begin{rem}
我们把定理~\ref{thm:fedsplit-correctness}应用到一个一般的最小二乘问题$f_k(\theta_k) = \frac{1}{2}\lVert A_k \theta_k + b_k \rVert^2,$ $A_k$列满秩，$k \in [K],$ 那么我们有
\begin{subequations}
\label{eq:lst-sol}
\begin{align}
\theta^*_{\texttt{FedGD}} & = \left( \sum\limits_{k=1}^K A_k^TA_k G \right)^{-1}\left( \sum\limits_{k=1}^K GA_k^Tb_k \right), \label{eq:lst-sol-fedgd} \\
\theta^*_{\texttt{FedProx}} & = \left( \sum\limits_{k=1}^K \left( I - P_k \right) \right)^{-1} \left( \sum\limits_{k=1}^K Q_kA_k^Tb_k \right)， \label{eq:lst-sol-fedprox}
\end{align}
\end{subequations}
上式中的$\theta^*_{\texttt{FedGD}}, \theta^*_{\texttt{FedProx}}$分别是算法\texttt{FedGD}以及\texttt{FedProx}的理论收敛点，矩阵
\begin{equation*}
G = \sum\limits_{i=0}^{R-1} ( I - \frac{1}{\mu} A_k^TA_k )^i, ~~ P_k = ( I + \frac{1}{\mu} A_k^TA_k )^{-1}, ~~ Q_k = (\mu I + A_k^TA_k)^{-1}.
\end{equation*}
我们把这个结论应用到例~\ref{eg:correctness}~即可得到联邦临近点算法的收敛结果~\eqref
{eq:eg-correctness-fedprox}。但是，从另一方面来说，最小二乘问题
\begin{equation*}
\text{minimize} ~ \sum\limits_{k=1}^K \frac{1}{2}\lVert A_k \theta_k + b_k \rVert^2
\end{equation*}
的最优解很容易算得是
\begin{equation*}
\theta^* = \left( \sum_{k=1}^K A_k^TA_k \right)^{-1} \sum_{k=1}^K A_k^Tb_k,
\end{equation*}
一般情况下，这与联邦平均算法\texttt{FedAvg}的理论收敛点~\eqref{eq:lst-sol-fedgd}~以及联邦临近点算法\texttt{FedProx}的的理论收敛点~\eqref{eq:lst-sol-fedprox}~并不一致。
\end{rem}

以上便是文献\parencite{pathak2020fedsplit}首先发现的大部分已有的联邦学习算法在收敛结果正确性方面的缺陷，因此需要需要重新考虑的优化问题~\eqref{eq:fl-basic}~的等价转换。我们把这个问题改写为如下的共识优化问题(注意与式~\eqref{eq:fl-basic-constraint}~的细微差别)
\begin{equation}
\label{eq:fedsplit-constraint}
\begin{array}{cl}
\minimize & F(\Theta) := \sum\limits_{k=1}^K w_k f_k(\theta_k), \\
\text{subject to} & A \Theta = 0,
\end{array}
\end{equation}
其中
\begin{equation*}
A = \begin{pmatrix} I_d & -I_d & & & \\ & I_d & -I_d & & \\ & & \ddots & \ddots & \\ & & & \ddots & -I_d \\ -I_d & & & & I_d \end{pmatrix}, ~ \Theta = \begin{pmatrix} \theta_1 \\ \vdots \\ \theta_K \end{pmatrix}, ~ \theta_1, \ldots, \theta_K \in \R^d.
\end{equation*}
约束优化问题~\eqref{eq:fedsplit-constraint}~的拉格朗日函数为
\begin{equation}
\label{eq:fedsplit-lagrangian}
\mathcal{L}(\Theta, \Lambda) = F(\Theta) - \langle \Lambda, A\Theta \rangle = \sum\limits_{k=1}^K w_k f_k(\theta_k) - \langle \Lambda, A\Theta \rangle,
\end{equation}
其中$\Lambda = \begin{pmatrix} \lambda_1 \\ \vdots \\ \lambda_K \end{pmatrix} \in \R^{Kd}$为对偶变量。考虑上述问题的一阶最优性条件$\nabla F(\Theta) - A^T \Lambda,$ 即
\begin{equation*}
\nabla F(\Theta) - \begin{pmatrix} \lambda_1 - \lambda_K \\ \lambda_2 - \lambda_1 \\ \vdots \\ \lambda_K - \lambda_{K-1} \end{pmatrix} = 0.
\end{equation*}
上式是一个所谓的单调包含问题(Monotone Inclusion Problem)
\begin{equation}
\label{eq:fedsplit-mono-incl}
0 \in \nabla F(\Theta) + \mathcal{N}_{\mathcal{E}}(\Theta)
\end{equation}
其中
\begin{equation}
\label{eq:normal-cone}
\mathcal{N}_{\mathcal{E}}(\Theta) = \begin{cases} \mathcal{E}^{\perp} & \text{ if } \Theta \in \mathcal{E} \\ \emptyset & \text{ otherwise } \end{cases}
\end{equation}
是凸集$\mathcal{E} = \{ \Theta \in \R^{Kd} ~ | ~ \theta_1 = \cdots = \theta_K\}$在点$\Theta$处的法锥(Normal Cone)，$\nabla F,$ $\mathcal{N}_{\mathcal{E}}$是两个极大单调算子(Maximal Monotone Operator)。

我们回忆一下极大单调算子以及法锥的定义：

\begin{definition}[极大单调算子(Maximal Monotone Operator)\cite{ryu2022large}]
\label{def:mmo}
$\R^n$上的一个算子(Operator) $\mathcal{T}: \R^n \rightrightarrows \R^n$指的是一个$\R^n \times \R^n$中的一个子集。$\R^n$中的一个算子$\mathcal{T}$被称作是一个单调算子(Monotone Operator)，如果$\mathcal{T}$满足下列条件
\begin{equation}
\label{eq:mo}
(u - v)^T (x - y) \geqslant 0, ~~ \forall ~ (x, u), (y, v) \in \mathcal{T},
\end{equation}
或者写成更紧凑的形式
\begin{equation}
\label{eq:mo-compact}
(\mathcal{T}x - \mathcal{T}y)^T (x - y) \geqslant 0, ~~ \forall ~ x, y \in \R^n.
\end{equation}
如果单调算子$\mathcal{T}$不真包含于任何一个单调算子，则称$\mathcal{T}$是一个极大单调算子(Maximal Monotone Operator)。
\end{definition}

\begin{definition}[法锥(Normal Cone)\cite{nocedal_2006_num_opt}]
\label{def:normal-cone}
设$\Omega \in \R^n$是$n$维欧氏空间中的非空凸集，$x \in \Omega$是其中一点，那么非空凸集$\Omega$在点$x$处的法锥(Normal Cone)为
\begin{equation}
\label{eq:def-normal-cone}
\mathcal{N}_{\Omega}(x) = \left\{ v ~ \middle| ~ \langle v, w - x \rangle \leqslant 0 ~ \forall w \in \Omega \right\}.
\end{equation}
\end{definition}

算子又被称作是集合值映射(Set-valued Mapping)，法锥也能视作一个算子，被称作法锥算子。法锥算子有另外一种表示，令$\iota_{\Omega}$为非空凸集$\Omega$的指示函数(Indicator Function)
\begin{equation*}
\iota_{\Omega} (x) = \begin{cases}
0, & \text{ 若 } x \in \Omega \\
\infty, & \text{ 若 } x \not\in \Omega
\end{cases}
\end{equation*}
那么
\begin{equation*}
\partial \iota_{\Omega}(x) = \mathcal{N}_{\Omega}(x),
\end{equation*}
其中$\partial$为指示函数$\iota_{\Omega}$的次微分。对于凸集$\mathcal{E}$中的点$\Theta \in \mathcal{E},$ 我们有
\begin{align*}
\mathcal{N}_{\mathcal{E}}(\Theta) & = \left\{ \omega \ \middle|\ \langle \omega, \widetilde{\Theta} - \Theta \rangle \leqslant 0, ~~ \forall \widetilde{\Theta} \in {\mathcal{E}} \right\} \\
& = \left\{ \omega \ \middle|\ \left\langle \sum\limits_{k=1}^K \omega_j, ~~ \widetilde{\Theta}_1 - \Theta_1 \right\rangle \leqslant 0, \ \forall \widetilde{\Theta}_1 \in \mathbb{R}^d \right\} \\
& = \left\{ \omega \ \middle|\ \sum\limits_{k=1}^K \omega_k = 0 \right\} = {\mathcal{E}}^{\perp},
\end{align*}
由此可以得到法锥$\mathcal{N}_{\mathcal{E}}(\Theta)$的具体表达式~\eqref{eq:normal-cone}。

对于极大单调算子，我们有如下的重要结果

\begin{prop}
\label{prop:os}
设算子$\mathcal{T} = \mathcal{A} + \mathcal{B},$ 其中$\mathcal{A}, \mathcal{B}$都是极大单调算子，$0 < s \in \R,$ 记
\begin{gather*}
\mathcal{R}_{s\mathcal{A}} = (\mathcal{I} + s \mathcal{A})^{-1}, \quad \mathcal{R}_{s\mathcal{B}} = (\mathcal{I} + s \mathcal{B})^{-1} \\
\mathcal{C}_{s\mathcal{A}} = 2 \mathcal{R}_{s\mathcal{A}} - \mathcal{I}, \quad \mathcal{C}_{s\mathcal{B}} = 2 \mathcal{R}_{s\mathcal{B}} - \mathcal{I}
\end{gather*}
那么我们有
\begin{itemize}
\item[(1)] $\mathcal{C}_{s\mathcal{A}}, \mathcal{C}_{s\mathcal{B}}, \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}$都是非扩张算子。我们称一个算子$\mathcal{T}$是非扩张的，若
\begin{equation*}
\lVert \mathcal{T}x - \mathcal{T}y \rVert \leqslant \lVert x - y \rVert, ~~ \forall ~ x, y \in \dom{\mathcal{T}} := \left\{ w \in \R^n ~ \middle| ~ \mathcal{T} w \neq \emptyset \right\}.
\end{equation*}
\item[(2)] $0\in \mathcal{A}(x) + \mathcal{B}(x) \Longleftrightarrow \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}(z) = z, \ x = \mathcal{R}_{s\mathcal{B}}(z).$
\end{itemize}
\end{prop}

\begin{proof}
(1). 任取$(x, u), (y, v) \in \mathcal{R}_{s\mathcal{A}} = (\mathcal{I} + s \mathcal{A})^{-1},$ 那么有
\begin{equation*}
x \in u + s\mathcal{A}(u), ~~ y \in v + s\mathcal{A}(v).
\end{equation*}
由于$\mathcal{A}$是单调算子，因此(由单调算子定义式~\eqref{eq:mo})有
\begin{equation*}
\frac{1}{s}((x - u) - (y - v))^T (u - v) \geqslant 0.
\end{equation*}
那么我们有
\begin{align*}
\lVert (2u - x) - (2v - y) \rVert^2 & = \lVert x - y \rVert^2 - 4 ((x - u) - (y - v))^T (u - v) \\
& \leqslant \lVert x - y \rVert^2,
\end{align*}
因此$\mathcal{C}_{s\mathcal{A}} = 2 \mathcal{R}_{s\mathcal{A}} - \mathcal{I}$是非扩张的算子。同理可证$\mathcal{C}_{s\mathcal{B}}$是非扩张的，从而$\mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}$也是非扩张算子。

(2). 我们有如下的等价关系
\begin{align*}
0 \in \mathcal{A}(x) + \mathcal{B}(x) & \Longleftrightarrow 0 \in (\mathcal{I} + s\mathcal{A})(x) - (\mathcal{I} - s\mathcal{B})(x) \\
& \Longleftrightarrow 0 \in (\mathcal{I} + s\mathcal{A})(x) - (2\mathcal{I} - (\mathcal{I} + s\mathcal{B}))(x) \\
& \Longleftrightarrow 0 \in (\mathcal{I} + s\mathcal{A})(x) - \mathcal{C}_{s\mathcal{B}} (\mathcal{I} + s\mathcal{B})(x) \\
& \Longleftrightarrow 0 \in (\mathcal{I} + s\mathcal{A})(x) - \mathcal{C}_{s\mathcal{B}} z, ~ z \in (\mathcal{I} + s\mathcal{B}) (x) \\
& \Longleftrightarrow \mathcal{C}_{s\mathcal{B}} (z) \in (\mathcal{I} + s\mathcal{A}) \mathcal{R}_{s\mathcal{B}} (z) , ~ x \in \mathcal{R}_{s\mathcal{B}} (z) \\
& \Longleftrightarrow \mathcal{R}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}} (z) = \mathcal{R}_{s\mathcal{B}} (z), ~ x \in \mathcal{R}_{s\mathcal{B}} (z) \\
& \Longleftrightarrow 2\mathcal{R}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}} (z) - \mathcal{C}_{s\mathcal{B}} (z) = 2 \mathcal{R}_{s\mathcal{B}} (z) - z + z, ~ x \in \mathcal{R}_{s\mathcal{B}} (z) \\
& \Longleftrightarrow \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}} (z) = z, ~ x \in \mathcal{R}_{s\mathcal{B}} (z)
\end{align*}
\end{proof}

当$s$明确取定的时候，我们可以把算子$\mathcal{R}_{s\mathcal{A}}, \mathcal{R}_{s\mathcal{B}}, \mathcal{C}_{s\mathcal{A}}, \mathcal{C}_{s\mathcal{B}}$分别简写为$\mathcal{R}_{\mathcal{A}}, \mathcal{R}_{\mathcal{B}}, \mathcal{C}_{\mathcal{A}}, \mathcal{C}_{\mathcal{B}}.$ 这个命题告诉了我们，求解单调包含问题$0 \in \mathcal{A}(x) + \mathcal{B}(x),$等价于求解非扩张算子$\mathcal{C}_{\mathcal{A}} \mathcal{C}_{\mathcal{B}}$的不动点问题。命题~\ref{prop:os}~所涉及到的算子$\mathcal{R}_{\mathcal{A}}, \mathcal{R}_{\mathcal{B}}$被称作算子$s\mathcal{A}, s\mathcal{B}$的预解式(Resolvent)，即算子$s\mathcal{A}, s\mathcal{B}$的逆算子；$\mathcal{C}_{\mathcal{A}}, \mathcal{C}_{\mathcal{B}}$被称作反射预解式(Reflected Resolvent)，或者Cayley算子。关于预解式相关的基础知识以及重要结果，读者可参阅\parencite[\S 2.5]{ryu2022large}。这样，我们就把我们需要求解的单调包含问题~\eqref{eq:fedsplit-mono-incl}~转化为了以下的不动点问题
\begin{equation}
\label{eq:fixed-pt-pr}
\mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}} (z) = z, ~ x \in \mathcal{R}_{s\mathcal{B}} (z).
\end{equation}
我们称这种转化为Peaceman–Rachford分裂(Peaceman–Rachford Splitting, PRS)。由于算子$\mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}$只是非扩张的，我们可以用$\tau$-平均算子$(1 - \tau) \mathcal{I} + \tau \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}$代替算子$\mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}},$ 其中$\tau \in (0, 1),$ 进而考虑如下的不动点问题
\begin{equation}
\label{eq:fixed-pt-general}
((1 - \tau) \mathcal{I} + \tau \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}) (z) = z, ~ x \in \mathcal{R}_{s\mathcal{B}} (z).
\end{equation}
特别地，当我们取$\tau = \frac{1}{2}$时，对应的不动点问题为
\begin{equation}
\label{eq:fixed-pt-dr}
\frac{1}{2}(\mathcal{I} + \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}) (z) = z, ~ x \in \mathcal{R}_{s\mathcal{B}} (z),
\end{equation}
称这种转化为Douglas-Rachford分裂(Douglas-Rachford Splitting, DRS)。

求解不动点问题~\eqref{eq:fixed-pt-pr}, \eqref{eq:fixed-pt-general}, \eqref{eq:fixed-pt-dr}~的一般方法是进行不动点迭代
\begin{equation}
\label{eq:fixed-pt-iter}
z^{(t+1)} = \mathcal{T}(z^{(t)}), ~~ \text{$\mathcal{T}$是某个非扩张算子。}
\end{equation}
具体到$\mathcal{T} = (1 - \tau) \mathcal{I} + \tau \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}},$ $\tau \in (0, 1],$ 当$\tau = 1$时，我们有Peaceman–Rachford分裂方法(Peaceman–Rachford Splitting Method, PRSM)
\begin{subequations}
\label{eq:prsm}
\begin{align}
x^{(t+1/2)} & = \mathcal{R}_{\mathcal{B}}(z^{(t)}) \label{eq:prsm-1} \\
z^{(t+1/2)} & = 2x^{(t+1/2)} - z^{(t)} \label{eq:prsm-2} \\
x^{(t+1)} & = \mathcal{R}_{\mathcal{A}}(z^{(t+1/2)}) \label{eq:prsm-3} \\
z^{(t+1)} & = z^{(t)} + 2x^{(t+1)} - 2x^{(t+1/2)} \label{eq:prsm-4}
\end{align}
\end{subequations}
当$\tau = \frac{1}{2}$时，Douglas–Rachford分裂方法(Douglas–Rachford Splitting Method, DRSM)
\begin{subequations}
\label{eq:drsm}
\begin{align}
x^{(t+1/2)} & = R_B(z^{(t)}) \label{eq:drsm-1} \\
z^{(t+1/2)} & = 2x^{(t+1/2)} - z^{(t)} \label{eq:drsm-2} \\
x^{(t+1)} & = R_A(z^{(t+1/2)}) \label{eq:drsm-3} \\
z^{(t+1)} & = z^{(t)} + x^{(t+1)} - x^{(t+1/2)} \label{eq:drsm-4}
\end{align}
\end{subequations}
我们注意到PRSM与DRSM的区别主要在迭代格式的最后一步~\eqref{eq:prsm-4}~与~\eqref{eq:drsm-4}，这对应的正是不动点迭代方法~\eqref{eq:fixed-pt-iter}~中算子$\mathcal{T}$的不同的选取。

我们现在回到我们想要求解的单调包含问题~\eqref{eq:fedsplit-mono-incl}，并将命题~\ref{prop:os}~应用到这个问题上。我们令
\begin{equation}
\label{eq:fedsplit-operator}
\mathcal{T} = \nabla F + \mathcal{N}_{\mathcal{E}}
\end{equation}
那么根据命题~\ref{prop:os}，我们将问题转化为了求算子$\mathcal{C}_{\mathcal{A}} \mathcal{C}_{\mathcal{B}}$的不动点的问题，其中$\mathcal{A} = \nabla F, \mathcal{B} = \mathcal{N}_{\mathcal{E}}.$ 对于这两个算子，我们有
\begin{equation*}
\mathcal{R}_{\nabla F} = \prox_{F, \mu}, \quad \mathcal{R}_{\mathcal{N}_E} = \Pi_{E},
\end{equation*}
其中$s = \frac{1}{\mu},$ 同时又有
\begin{align*}
\prox_{F, \mu}(\Theta) & = \argmin_{\omega} \left\{ F(\omega) + \frac{\mu}{2} \lVert \omega - \Theta \rVert^2 \right\} \\
& = \argmin_{\omega} \left\{ \sum_{k=1}^K f_k(\omega_k) + \frac{\mu}{2} \sum_{k=1}^K \lVert \omega_k - \theta_k \rVert^2 \right\} \\
& = (\prox_{f_k, \mu}(\theta_k))_{j=1}^K.
\end{align*}
基于以上的问题转换，文献\parencite{pathak2020fedsplit}将PRSM应用到了联邦学习关注的问题~\eqref{eq:fedsplit-constraint}，提出了联邦分裂算法\texttt{FedSplit}。该算法的伪代码见算法~\ref{algo:fedsplit}，其中的$\texttt{prox\_update}_k$是求解子节点$k$上的临近算子$\prox_{f_k, \mu}$的非精确的方法，例如随机梯度下降。

\input{algorithms/fedsplit.tex}

联邦分裂算法\texttt{FedSplit}很好地解决了在本节开头的例~\ref{eg:correctness}~以及定理~\ref{thm:fedsplit-correctness}~中提出的关于联邦学习算法普遍存在的收敛正确性相关的问题，即我们有如下的结论

\begin{theorem}
\label{thm:fedsplit-main}
待写。。。。
\end{theorem}

联邦分裂算法\texttt{FedSplit}有它自身的一些局限性，待写。。。

\input{algorithms/feddr.tex}
