\section{联邦学习中的临近点算法}
\addcontentsline{toe}{section}{{2.2\ \ Proximal Point Algorithms in Federated Learning}\numberline\,}
\label{sec:chap2-ppa}

% almost finished
% indexed

基于减小数据非独立同分布情形下，每一轮的子节点更新对整体模型的影响，文献\parencite{sahu2018fedprox}首先提在子节点上的局部模型中增加邻近项的联邦临近算法\texttt{FedProx}\index{联邦临近算法, \texttt{FedProx}}，达到了算法收敛更快、更加稳定的目的。相较于\texttt{SCAFFOLD}算法\ref{algo:scaffold}，使用临近项的方案不需要进行额外 (关于梯度信息) 参数的维护与传递，没有通信开销以及安全性上的额外代价。

具体来说，在算法迭代的第$t+1$轮，每个子节点优化的目标函数从$f_k(\theta_k)$变为了如下的带临近项的函数
\begin{equation}
\label{eq:fedprox}
h_k(\theta_k, \theta^{(t)}) := f_k(\theta_k) + \frac{\mu}{2} \lVert \theta_k - \theta^{(t)} \rVert^2,
\end{equation}
上式中的$\mu$被称作罚参数 (Penalty Constant)\index{罚参数, Penalty Constant}。值得注意的是，上式临近项的中心为$\theta^{(t)},$ 即上一轮迭代得到的全局模型。实际上，联邦临近算法\texttt{FedProx}整体的优化问题可以建模为如下的约束优化问题
\begin{equation}
\label{eq:fedprox-whole}
\begin{array}{cl}
\minimize & \frac{1}{K} \sum\limits_{k=1}^K \left\{ f_k(\theta_k) + \frac{\mu}{2} \lVert \theta_k - \theta \rVert^2 \right\} \\
\text{subject to} & \theta = \frac{1}{K} \sum\limits_{k=1}^K \theta_k.
\end{array}
\end{equation}
关于临近项中心的其他选取，文献\parencite{hanzely2020federated,li_2021_ditto}进行了研究，我们随后进行详细介绍。我们将联邦临近算法\texttt{FedProx}的伪代码总结在算法\ref{algo:fedprox}~中。

\input{algorithms/fedprox}

我们把$\gamma$-非精确解$\theta_k^{(t)}$记作
\begin{equation}
\label{eq:prox-op}
\theta_k^{(t)} \approx \prox_{f_k, \mu} (\theta^{(t)}) := \argmin\limits_{\theta_k} \left\{ f_k(\theta_k) + \frac{\mu}{2} \lVert \theta_k - \theta^{(t)} \rVert^2 \right\},
\end{equation}
其中的$\prox_{f_k, \mu}$为临近算子 (Proximity Operator)\cite{Moreau_1965_prox}\index{临近算子, Proximity Operator}。令$s = \frac{1}{\mu},$ 因为$\prox_{f_k, \mu} = \prox_{sf_k, 1},$ 我们也把$\prox_{f_k, \mu}$记为$\prox_{sf_k}.$ 相应的函数
\begin{equation}
\label{eq:moreau_env}
\mathcal{M}_{sf_k} (\theta^{(t)}) = \mathcal{M}_{f_k, \mu} (\theta^{(t)}) := \inf\limits_{\theta_k} \left\{ f_k(\theta_k) + \frac{\mu}{2} \lVert \theta_k - \theta^{(t)} \rVert^2 \right\}
\end{equation}
被称作$f_k$的Moreau包络 (Moreau Envelope)\index{Moreau包络, Moreau Envelope} 或者Moreau-Yosida正规化 (Moreau-Yosida Regularization)\index{Moreau-Yosida正规化, Moreau-Yosida Regularization}。函数$f_k$的Moreau包络与它的临近算子有如下关系\cite{Parikh_2014_pa}
\begin{equation}
\label{eq:prox-moreau-relation}
\prox_{sf_k} (\theta) = \theta - s \nabla \mathcal{M}_{sf_k} (\theta), ~ \forall \theta \in \R^d,
\end{equation}
也就是说，$\prox_{sf_k}$可以视作是以$s$为步长进行的极小化函数$\mathcal{M}_{sf_k}$的梯度下降算子。

关于联邦临近算法\texttt{FedProx}在非凸情况下的收敛性，文献\parencite{sahu2018fedprox}有如下的结论
\begin{theorem}[\parencite{sahu2018fedprox} Theorem 4]
\label{thm:fedprox}
假设子节点的目标函数$\{f_k\}_{k=1}^K$都是非凸、$L$−光滑(定义见式~\eqref{eq:l-smooth})函数，并且存在常数$L_- > 0,$使得$\nabla^2 f_k \succcurlyeq -L_- I_d.$ 又假设$\{f_k\}_{k=1}^K$满足差异有界条件 (Bounded Dissimilarity)，即对$\varepsilon > 0,$ 存在常数$B_{\varepsilon} > 0,$ 使得集合$\mathcal{S}_{\varepsilon}^c := \{ \theta ~|~ \lVert \nabla f(\theta) \rVert^2 > \varepsilon\}$中的任何一点$\theta$都满足不等式
\begin{equation}
\label{eq:fedprox_bdd_dissim}
B(\theta) := \frac{\expectation_k [\lVert \nabla f_k(\theta) \rVert^2]}{\lVert \nabla f(\theta) \rVert^2} \leqslant B_{\varepsilon}.
\end{equation}
选取常数$\mu, \gamma$满足
\begin{equation*}
\rho := \left( \frac{1}{\mu} - \frac{\gamma B}{\mu} - \frac{B(1+\gamma)\sqrt{2}}{\bar{\mu}\sqrt{K}} - \frac{LB(1+\gamma)}{\bar{\mu}\mu} - \frac{LB^2(1+\gamma)^2}{2\bar{\mu}^2} - \frac{LB^2(1+\gamma)^2}{\bar{\mu}^2 K} \left( 2\sqrt{2K} + 2 \right) \right) > 0,
\end{equation*}
其中$\bar{\mu} = \mu - L_- > 0.$ 那么在联邦临近算法\ref{algo:fedprox}~的第$t+1$步，设前一步的全局模型参数$\theta^{(t)}$为全局目标函数$f$的一阶非稳定点 (即$\theta^{(t)} \in \mathcal{S}_{\varepsilon}^c$)，那么$f$的值满足如下的下降关系
\begin{equation*}
\expectation\nolimits_{\mathcal{S}^{(t)}}[f(\theta^{(t+1)})] \leqslant f(\theta^{(t)}) - \rho \lVert \nabla f (\theta^{(t)}) \rVert^2.
\end{equation*}
\end{theorem}

\begin{rem}
对于联邦临近算法\texttt{FedProx}收敛性定理\ref{thm:fedprox}，我们有如下的观察：在$\lVert \nabla f \rVert$的零点附近，如果这个零点没有被$\mathbb{E}_k[\lVert \nabla f_k \rVert]$消除掉，即这个点也是$\mathbb{E}_k[\lVert \nabla f_k \rVert]$的零点，且零点的阶数相同或更高，那么在这个点的邻域内，$B_{\varepsilon}$会随着$\varepsilon \to 0$而急速趋向于无穷，导致在$\lVert \nabla f \rVert$的零点附近，$\rho > 0$的假设不再成立，那么定理中的不等式就变得无意义。

当子节点之间的数据分布完全一致的时候 (理想情况下)，$B_{\varepsilon}$恒为1，就不会有这个问题。这也是后续一系列文章\cite{pathak2020fedsplit,tran2021feddr}进行改进的出发点。
\end{rem}

联邦临近算法\texttt{FedProx}的积极意义在于，首次将临近点算法 (Proximal Point Algorithms, PPA)\index{临近点算法, Proximal Point Algorithms, PPA)} 引入联邦学习领域中，虽然临近点算法只是用做了 (内循环问题的) 子节点上的子问题的求解器，而\texttt{FedProx}算法整体严格意义上来看并不是临近点算法。临近点算法不仅仅在理论分析上可以提供一个好的框架，在算法设计上也能提供一个支撑。后续在\$\ref{chap3}~中我们将要详细研究的有关个性化联邦学习的工作\cite{hanzely2020federated,acar2021feddyn,li_2021_ditto,t2020pfedme,li2021pfedmac}都依赖于引入临近项 (或者类似的项) 作为模型个性化的主要技术手段。关于临近点算法本身在联邦学习的进一步研究，也有将临近点算法与方差缩减技术相结合(\texttt{FedProxVR}算法)\cite{Dinh_2020_FL_PSVRG}等方面的工作。

