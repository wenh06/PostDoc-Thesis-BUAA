\section{联邦学习中的优化算法}
\addcontentsline{toe}{section}{{2.1\ \ Overview of Optimization Algorithms in Federated Learning}\numberline\,}
\label{sec:chap2-overview}

优化算法自从联邦学习这一概念诞生起，便一直是相关研究的中心问题之一。联邦学习开创性的文献\cite{mcmahan2017fed_avg}中，最重要也是最为人所熟知的便是优化算法\texttt{FederatedAveraging}（简记为\texttt{FedAvg}）的提出。这种算法的核心思想，是充分利用子节点的计算能力，在执行完一定轮数的随机梯度下降(Stochastic Gradient Descent, SGD)之后，由中心节点收集各子节点发送过来的模型参数，进行平均(Averaging)，之后再将平均之后的模型参数广播(Broadcast)给子节点进行下一轮迭代。这样，规避了每一轮SGD都进行通信带来的巨大的通信开销，在加速模型训练的同时，数值上在某些情况下还有更好的收敛性。这实际上是一种比较朴素与简单的``跳步''的思想，这种思想也在随后的研究\cite{zhang2020fedpd, proxskip, proxskip-vr}也得到了进一步的发展。

联邦学习优化算法最重要的设计原则，是计算效率以及通信效率并重，甚至很多时候通信效率是更重要的一个方面。这也是联邦学习与传统的分布式优化(Distributed Optimization)最显著的区别之一。实际上，分布式优化在联邦学习被提出之前就是一个被研究得比较多的问题，从具体的问题，例如分布式主成分分析(Principal Component Analysis, PCA)\cite{dist_pca_2014_nips}，到一般的算法理论\cite{boyd2011distributed}都有研究人员进行了研究。当时这些研究往往仅从计算效率以及效果出发，往往不考虑通信效率、通信保密性等问题。文献\cite{mcmahan2017fed_avg}正是从实际问题出发，发现了这些需求，基于一种分布式梯度下降算法\cite{chen2016_revisit}(该算法在文献\cite{mcmahan2017fed_avg}中被称作\texttt{FedSGD}算法)，做了上文提到的改进而提出了\texttt{FedAvg}算法。

这里还需要强调的是，\texttt{FedAvg}相对于\texttt{FedSGD}的另一个关键的改进是，子节点与中心节点之间传输的信息，从梯度改为了模型参数。这在某种程度上规避了从梯度泄露联邦学习参与方私密训练数据\cite{zhu2019deep_leakage}的潜在风险。

沿用式\eqref{eq:general-hfl}中的记号，本文考虑联邦学习中的优化问题，其最基本的格式如下
\begin{equation}
\label{eq:fl-basic-dist}
\begin{array}{cl}
\minimize\limits_{\theta \in \R^d} & f(\theta) = \expectation\limits_{k \sim {\mathcal{P}}} [f_k(\theta)], \\
\text{where} & f_k(\theta) = \expectation\limits_{(x, y) \sim \mathcal{D}_k} [\ell_k(\theta; x, y)],
\end{array}
\end{equation}
假设我们令$\mathcal{P} = \{1, 2, \ldots, K\},$ 则上述模型可以简记为
\begin{equation}
\label{eq:fl-basic}
\begin{array}{cl}
\minimize\limits_{\theta \in \R^d} & f(\theta) = \sum\limits_{k=1}^K w_k f_k(\theta).
\end{array}
\end{equation}
对于$f$以及$f_k,$ 我们一般都假设它满足如下几条最基本的性质
\begin{itemize}
    \item[(A1)] $f$以及$f_k$都是$L-$光滑的($L-$smooth)，即
    \begin{equation}
    \label{eq:l-smooth}
    \begin{array}{c}
    \lVert \nabla f (\theta) - f (\theta') \rVert \leqslant L \lVert \theta - \theta' \rVert, \\
    \lVert \nabla f_k (\theta) - f_k (\theta') \rVert \leqslant L \lVert \theta - \theta' \rVert,
    \end{array}
    ~ \forall \theta, \theta' \in \R^d, k = 1, \ldots, K.
    \end{equation}
    \item[(A2)] $f$下有界(Lower Bounded)：存在常数$c \in \R,$ 使得
    \begin{equation}
    \label{eq:lower-bounded}
    f(\theta) \geqslant c > -\infty, ~ \forall \theta \in \R^d.
    \end{equation}
    很多时候，为了方便收敛性的分析，
    \item[(A2)] 
\end{itemize}

更方便与自然的做法，是将问题\eqref{eq:fl-basic}写成约束优化问题的格式：
\begin{equation}
\label{eq:fl-basic-constraint}
\begin{array}{cl}
\begin{array}{cl}
\minimize\limits_{\theta \in \R^d} & f(\theta) = \expectation\limits_{k \sim {\mathcal{P}}} [f_k(\theta)], \\
\text{where} & f_k(\theta) = \expectation\limits_{(x, y) \sim \mathcal{D}_k} [\ell_k(\theta; x, y)],
\end{array}
\end{array}
\end{equation}

这里还应当着重指出的是，待写。。。。
