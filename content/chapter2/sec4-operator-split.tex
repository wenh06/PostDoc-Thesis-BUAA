\section{联邦学习中的算子分裂算法}
\addcontentsline{toe}{section}{{2.3\ \ Operator Splitting Algorithms in Federated Learning}\numberline\,}
\label{sec:chap2-operator-split}

% almost finished
% indexed

我们已经讨论过的通用加速技巧\cite{reddi2020fed_opt}，方差缩减技术\cite{karimireddy2020scaffold}，以及临近点方法\cite{sahu2018fedprox}，解决的都是联邦学习在各种各样场景下的收敛性、收敛率相关的问题，而算法收敛结果的正确性这一问题始终没有得到重视。我们来举一个非常简单的例子来说明这个问题。

\begin{example}
\label{eg:correctness}
假设我们要拟合线性模型$y = mx$, $m$为需要拟合的变量，使用均方误差 (Mean Squared Error, MSE)\index{均方误差, Mean Squared Error, MSE} 做损失函数(目标函数)，即我们考虑的是一个最小二乘问题 (Least Square Problem, LSP)\index{最小二乘问题, Least Square Problem, LSP)}。假设我们有两个参与方，参与方1的拥有的训练数据为$\{ (0, 2), (1, 2) \},$ 相应的目标函数为
\begin{equation*}
f_1(m_1) = \frac{1}{2} (4 + (m_1 - 2)^2);
\end{equation*}
参与方2的拥有的训练数据为$\{ (2, 0), (2, 1) \},$ 相应的目标函数为
\begin{equation*}
f_2(m_2) = \frac{1}{2} (4m_2^2 + (2m_2 - 1)^2).
\end{equation*}
那么参与方1单独拟合结果为$m_1 = 2$, 参与方2单独拟合结果为$m_2 = \frac{1}{4}$, 采用联邦平均算法\texttt{FedAvg}的结果是$m = \frac{9}{8}.$ 而采用联邦临近算法\texttt{FedProx}的结果(这里，我们可以直接对带临近项的目标函数求极小，也可以采用定理~\ref{thm:fedsplit-correctness}~中的一般计算公式)是
\begin{equation}
\label{eq:eg-correctness-fedprox}
m = \frac{18 + 4\mu}{16 + 9\mu},
\end{equation}
其中$\mu$为临近项相关的系数 (见式~\eqref{eq:fedprox})。

数据都拿一起拟合结果为$k = \frac{4}{9}.$ 注意，这里一起拟合的结果并不等于单独拟合的均值 (\texttt{FedAvg}的结果)，原因就在于目标函数里的k是2次的，不是线性的。
\end{example}

文献\cite{pathak2020fedsplit}首先注意到这个问题，并具体分析了联邦平均算法\texttt{FedAvg}与联邦临近算法\texttt{FedProx}的理论收敛结果，如下

\begin{theorem}
\label{thm:fedsplit-correctness}
假设我们有$K$个节点参与联邦学习，每一个节点$k \in [K]$的目标函数为$f_k(\theta_k),$ 这些函数都是有限凸函数，而且都是$L$-光滑的(定义见式~\eqref{eq:l-smooth})，那么
\begin{itemize}
\item[(1)] 若采用联邦平均算法\texttt{FedAvg} \ref{algo:fedavg}，且假设子节点上的参数更新方式为全量的梯度下降。我们把这种简化的算法记为\texttt{FedGD}，记梯度映射为
\begin{equation}
\label{eq:grad-mapping}
\mathcal{G}_k (\theta_k) := \theta_k - \eta \nabla f_k (\theta_k),
\end{equation}
并记$\mathcal{G}^i_k (\theta_k) := \underbrace{G_k\circ\cdots\circ G_k}_{i-\text{次复合}} (\theta_k).$ 若算法 \ref{algo:fedavg}~生成的全局模型参数的序列$\{ \theta^{(t)} \}_{t=1}^{\infty}$收敛，那么所有子节点$k \in [K]$模型参数的序列$\{ \theta_k^{(t)} \}_{t=1}^{\infty}$有公共的极限$\theta^*,$ 且$\theta^*$满足下列不动点条件
\begin{equation}
\label{eq:fedgd-fixed-pt}
\sum\limits_{i=1}^R \sum\limits_{k=1}^K \nabla f_k(\mathcal{G}_k^{i-1}(\theta^*)) = 0.
\end{equation}
\item[(2)] 若采用联邦临近算法\texttt{FedProx} \ref{algo:fedprox}，记Moreau包络映射~\eqref{eq:moreau_env}~为
\begin{equation}
\label{eq:moreau-mapping}
\mathcal{M}_{f_k, \mu} (\theta) := \inf\limits_{\theta_k} \left\{ f_k(\theta_k) + \frac{\mu}{2} \lVert \theta - \theta_k \rVert^2 \right\}.
\end{equation}
若算法 \ref{algo:fedprox}~生成的全局模型参数的序列$\{ \theta^{(t)} \}_{t=1}^{\infty}$收敛，那么所有子节点$k \in [K]$模型参数的序列$\{ \theta_k^{(t)} \}_{t=1}^{\infty}$有公共的极限$\theta^*,$ 且$\theta^*$满足下列不动点条件
\begin{equation}
\label{eq:fedprox-fixed-pt}
\sum\limits_{k=1}^K \nabla \mathcal{M}_{f_k, \mu} (\theta^*) = 0.
\end{equation}
\end{itemize}
\end{theorem}

\begin{proof}
\begin{itemize}
\item[(1)] 记$\Theta^{(t)} = (\theta_1^{(t)}, \cdots, \theta_K^{(t)}).$ 由于算法 \ref{algo:fedavg}~生成的全局模型参数的序列$\{ \theta^{(t)} \}_{t=1}^{\infty}$收敛，也就是说约束优化问题~\eqref{eq:fedavg-constraint}~有解$\Theta^* = (\theta_1^*, \cdots, \theta_K^*).$ 这个解要满足问题~\eqref{eq:fedavg-constraint}~的约束条件，即满足
\begin{equation*}
\theta_1^* = \cdots = \theta_K^*,
\end{equation*}
那么我们就知道了所有子节点$k \in [K]$模型参数的序列$\{ \theta_k^{(t)} \}_{t=1}^{\infty}$有公共的极限$\theta^* = \theta_1^* = \cdots = \theta_K^*.$ 全局模型参数序列
\begin{equation*}
\theta^{(t)} = \frac{1}{K} (\theta_1^{(t)} + \cdots + \theta_K^{(t)}), ~~ t = 1, \ldots
\end{equation*}
也收敛于$\theta^*,$ 从而有
\begin{equation*}
\theta^* = \texttt{FedGD} (\theta^*) := \frac{1}{K} ( \mathcal{G}^R_1 (\theta^*) + \cdots + \mathcal{G}^R_K (\theta^*) )
\end{equation*}
即
\begin{equation*}
0 = \theta^* - \frac{1}{K} \sum\limits_{k=1}^K \mathcal{G}^R_k (\theta^*)
\end{equation*}
我们把梯度映射~\eqref{eq:grad-mapping}~的定义反复代入上式中，即有
\begin{align*}
0 = & \frac{1}{K}\sum_{k=1}^K \mathcal{G}^R_k(\theta^{*}) - \theta^{*} = \frac{1}{K} \sum_{k=1}^K \mathcal{G}_k ( \mathcal{G}^{R-1}_k (\theta^{*}) ) - \theta^{*} \\
= & \frac{1}{K}\sum_{k=1}^K \left( \mathcal{G}^{R-1}_k(\theta^{*}) - \eta\nabla f_k (\mathcal{G}^{R-1}_k(\theta^{*})) \right) - \theta^{*} \\
= & \frac{1}{K}\sum_{k=1}^K \mathcal{G}^{R-1}_k(\theta^{*}) - \theta^{*} - \frac{\eta}{K} \sum_{k=1}^K \nabla f_k (\mathcal{G}^{R-1}_k(\theta^{*})) \\
& \hspace{3em} \vdots \\
= & \frac{1}{K}\sum_{k=1}^K \mathcal{G}^{0}_k(\theta^{*}) - \theta^{*} - \frac{\eta}{K} \sum\limits_{i=1}^R \sum\limits_{k=1}^K \nabla f_k( \mathcal{G}_k^{i-1}(\theta^*) ) \\
= & - \frac{\eta}{K} \sum\limits_{i=1}^R \sum\limits_{k=1}^K \nabla f_k( \mathcal{G}_k^{i-1}(\theta^*) ).
\end{align*}
这样，我们就证明了$\theta^*$满足不动点条件~\eqref{eq:fedgd-fixed-pt}.
\item[(2)] 关于联邦临近算法\texttt{FedProx}~\ref{algo:fedprox}~在所有子节点上产生的模型参数序列有公共极限$\theta^*$的论断同(1)可证，我们来证明这个极限满足不动点条件~\eqref{eq:fedprox-fixed-pt}。由于$f_k$是光滑凸函数，因此由~\eqref{eq:prox-moreau-relation}~有
\begin{equation*}
\prox_{f_k, \mu} (\theta) = \theta - \frac{1}{\mu} \nabla \mathcal{M}_{f_k, \mu}(\theta),
\end{equation*}
于是我们有
\begin{align*}
0 & = \theta^* - \frac{1}{K}\sum_{k=1}^K \prox_{f_k, \mu}(\theta^*) \\
& = \theta^* - \frac{1}{K} \sum_{k=1}^K \left( \theta^* - \frac{1}{\mu} \nabla \mathcal{M}_{f_k, \mu}(\theta^*) \right) \\
& = \theta^* - \frac{1}{K} \sum_{k=1}^K \theta^* + \frac{1}{\mu K} \sum_{k=1}^K \nabla \mathcal{M}_{f_k, \mu}(\theta^*) \\
& = \frac{1}{\mu K} \sum_{k=1}^K \nabla \mathcal{M}_{f_k, \mu}(\theta^*).
\end{align*}
\end{itemize}
\end{proof}

\begin{rem}
我们把定理~\ref{thm:fedsplit-correctness}应用到一个一般的最小二乘问题$f_k(\theta_k) = \frac{1}{2}\lVert A_k \theta_k + b_k \rVert^2,$ $A_k$列满秩，$k \in [K],$ 那么我们有
\begin{subequations}
\label{eq:lst-sol}
\begin{align}
\theta^*_{\texttt{FedGD}} & = \left( \sum\limits_{k=1}^K A_k^TA_k G \right)^{-1}\left( \sum\limits_{k=1}^K GA_k^Tb_k \right), \label{eq:lst-sol-fedgd} \\
\theta^*_{\texttt{FedProx}} & = \left( \sum\limits_{k=1}^K \left( I - P_k \right) \right)^{-1} \left( \sum\limits_{k=1}^K Q_kA_k^Tb_k \right)， \label{eq:lst-sol-fedprox}
\end{align}
\end{subequations}
上式中的$\theta^*_{\texttt{FedGD}}, \theta^*_{\texttt{FedProx}}$分别是算法\texttt{FedGD}以及\texttt{FedProx}的理论收敛点，矩阵
\begin{equation*}
G = \sum\limits_{i=0}^{R-1} ( I - \frac{1}{\mu} A_k^TA_k )^i, ~~ P_k = ( I + \frac{1}{\mu} A_k^TA_k )^{-1}, ~~ Q_k = (\mu I + A_k^TA_k)^{-1}.
\end{equation*}
我们把这个结论应用到例~\ref{eg:correctness}~即可得到联邦临近算法的收敛结果~\eqref{eq:eg-correctness-fedprox}。但是，从另一方面来说，最小二乘问题
\begin{equation*}
\text{minimize} ~ \sum\limits_{k=1}^K \frac{1}{2}\lVert A_k \theta_k + b_k \rVert^2
\end{equation*}
的最优解很容易算得是
\begin{equation*}
\theta^* = \left( \sum_{k=1}^K A_k^TA_k \right)^{-1} \sum_{k=1}^K A_k^Tb_k,
\end{equation*}
一般情况下，这与联邦平均算法\texttt{FedAvg}的理论收敛点~\eqref{eq:lst-sol-fedgd}~以及联邦临近算法\texttt{FedProx}的的理论收敛点~\eqref{eq:lst-sol-fedprox}~并不一致。
\end{rem}

以上便是文献\parencite{pathak2020fedsplit}首先发现的大部分已有的联邦学习算法在收敛结果正确性方面的缺陷，因此需要需要重新考虑的优化问题~\eqref{eq:fl-basic}~的等价转换。我们再回到约束优化问题~\eqref{eq:fedavg-constraint}~的格式
\begin{equation*}
\begin{array}{cl}
\minimize & F(\Theta) := \frac{1}{K}  \sum\limits_{k=1}^K f_k(\theta_k), \\
\text{subject to} & \Theta = \col(\theta_1, \cdots, \theta_K) \in \mathcal{E}，
\end{array}
\end{equation*}
我们通过使用$\mathcal{E}$的指示函数 (Indicator Function)\index{指示函数, Indicator Function)} 作为罚函数的方式将以上问题转换为无约束优化问题
\begin{equation}
\label{eq:fedsplit-pen}
\minimize \quad F(\Theta) + \iota_{\mathcal{E}}(\Theta) = \frac{1}{K} \sum\limits_{k=1}^K f_k(\theta_k) + \iota_{\mathcal{E}}(\Theta).
\end{equation}
对于一个一般的非空凸集$\Omega,$ 其指示函数$\iota_{\Omega}$的定义为
\begin{equation*}
\iota_{\Omega} (x) = \begin{cases}
0, & \text{ 若 } x \in \Omega \\
\infty, & \text{ 若 } x \not\in \Omega
\end{cases}
\end{equation*}
无约束优化问题~\eqref{eq:fedsplit-pen}~的一阶最优性条件为
\begin{equation}
\label{eq:fedsplit-mono-incl-pre}
0 \in \nabla F(\Theta) + \partial \iota_{\mathcal{E}}(\Theta)
\end{equation}
上式是一个所谓的单调包含问题 (Monotone Inclusion Problem, MIP)\index{单调包含问题, Monotone Inclusion Problem, MIP}，其中$\partial$为指示函数$\iota_{\mathcal{E}}$的次微分，并且有
\begin{equation}
\label{eq:normal-cone}
\partial \iota_{\mathcal{E}}(\Theta) = \mathcal{N}_{\mathcal{E}}(\Theta) = \begin{cases} \mathcal{E}^{\perp} & \text{ if } \Theta \in \mathcal{E} \\ \emptyset & \text{ otherwise } \end{cases}
\end{equation}
是凸集$\mathcal{E} = \{ \Theta \in \R^{Kd} ~ | ~ \theta_1 = \cdots = \theta_K\}$在点$\Theta$处的法锥 (Normal Cone)，$\nabla F,$ $\mathcal{N}_{\mathcal{E}}$是两个极大单调算子 (Maximal Monotone Operator)。单调包含问题~\eqref{eq:fedsplit-mono-incl-pre}~可进一步改写为
\begin{equation}
\label{eq:fedsplit-mono-incl}
0 \in \nabla F(\Theta) + \mathcal{N}_{\mathcal{E}}(\Theta)
\end{equation}

我们回忆一下极大单调算子以及法锥等基本概念：

\begin{definition}[极大单调算子 (Maximal Monotone Operator)\cite{zarantonello1960solving,Minty_1962,ryu2022large}]
\label{def:mmo}
$\R^n$上的一个算子(Operator) $\mathcal{T}: \R^n \rightrightarrows \R^n$指的是一个$\R^n \times \R^n$中的一个子集。$\R^n$中的一个算子$\mathcal{T}$被称作是一个单调算子 (Monotone Operator)\index{单调算子,  Monotone Operator}，如果$\mathcal{T}$满足下列条件
\begin{equation}
\label{eq:mo}
(u - v)^T (x - y) \geqslant 0, ~~ \forall ~ (x, u), (y, v) \in \mathcal{T},
\end{equation}
或者写成更紧凑的形式
\begin{equation}
\label{eq:mo-compact}
(\mathcal{T}x - \mathcal{T}y)^T (x - y) \geqslant 0, ~~ \forall ~ x, y \in \R^n.
\end{equation}
如果单调算子$\mathcal{T}$不真包含于任何一个单调算子，则称$\mathcal{T}$是一个极大单调算子 (Maximal Monotone Operator)\index{极大单调算子, Maximal Monotone Operator}。
\end{definition}

\begin{definition}[法锥 (Normal Cone)\cite{nocedal_2006_num_opt}]
\label{def:normal-cone}
设$\Omega \in \R^n$是$n$维欧氏空间中的非空凸集，$x \in \Omega$是其中一点，那么非空凸集$\Omega$在点$x$处的法锥 (Normal Cone)\index{法锥, Normal Cone} 为
\begin{equation}
\label{eq:def-normal-cone}
\mathcal{N}_{\Omega}(x) = \left\{ v ~ \middle| ~ \langle v, w - x \rangle \leqslant 0 ~ \forall w \in \Omega \right\}.
\end{equation}
\end{definition}

算子又被称作是集值映射 (Set-Valued Mapping)\index{集值映射, Set-Valued Mapping}，法锥也能视作一个算子，被称作法锥算子。对于一个一般的非空凸集$\Omega$以及$x \in \Omega,$ 我们有
\begin{align*}
\mathcal{N}_{\Omega}(x) & = \left\{ \omega \ \middle|\ \langle \omega, \widetilde{x} - x \rangle \leqslant 0, ~~ \forall \widetilde{x} \in \Omega \right\} \\
& = \left\{ \omega \ \middle|\ \langle \omega, \widetilde{x} - x \rangle + \iota_{\Omega}(x) \leqslant \iota_{\Omega}(\widetilde{x}), ~~ \forall \widetilde{x} \in \Omega \right\} \\
& = \left\{ \omega \ \middle|\ \langle \omega, \widetilde{x} - x \rangle + \iota_{\Omega}(x) \leqslant \iota_{\Omega}(\widetilde{x}), ~~ \forall \widetilde{x} \in \R^d \right\} \\
& = \partial \iota_{\Omega}(x).
\end{align*}
同时，对于凸集$\mathcal{E}$中的点$\Theta \in \mathcal{E},$ 我们有
\begin{align*}
\mathcal{N}_{\mathcal{E}}(\Theta) & = \left\{ \omega \ \middle|\ \langle \omega, \widetilde{\Theta} - \Theta \rangle \leqslant 0, ~~ \forall \widetilde{\Theta} \in {\mathcal{E}} \right\} \\
& = \left\{ \omega \ \middle|\ \left\langle \sum\limits_{k=1}^K \omega_j, ~~ \widetilde{\Theta}_1 - \Theta_1 \right\rangle \leqslant 0, \ \forall \widetilde{\Theta}_1 \in \mathbb{R}^d \right\} \\
& = \left\{ \omega \ \middle|\ \sum\limits_{k=1}^K \omega_k = 0 \right\} = {\mathcal{E}}^{\perp},
\end{align*}
由此可以得到指示函数$\iota_{\mathcal{E}}$的次微分以及法锥$\mathcal{N}_{\mathcal{E}}(\Theta)$的具体表达式~\eqref{eq:normal-cone}。

对于极大单调算子，我们有如下的重要结果

\begin{prop}
\label{prop:os}
设算子$\mathcal{T} = \mathcal{A} + \mathcal{B},$ 其中$\mathcal{A}, \mathcal{B}$都是极大单调算子，$0 < s \in \R,$ 记
\begin{gather*}
\mathcal{R}_{s\mathcal{A}} = (\mathcal{I} + s \mathcal{A})^{-1}, \quad \mathcal{R}_{s\mathcal{B}} = (\mathcal{I} + s \mathcal{B})^{-1} \\
\mathcal{C}_{s\mathcal{A}} = 2 \mathcal{R}_{s\mathcal{A}} - \mathcal{I}, \quad \mathcal{C}_{s\mathcal{B}} = 2 \mathcal{R}_{s\mathcal{B}} - \mathcal{I}
\end{gather*}
那么我们有
\begin{itemize}
\item[(1)] $\mathcal{C}_{s\mathcal{A}}, \mathcal{C}_{s\mathcal{B}}, \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}$都是非扩张算子 (Nonexpansive Operator)\index{非扩张算子, Nonexpansive Operator}。我们称一个算子$\mathcal{T}$是非扩张的，若
\begin{equation*}
\lVert \mathcal{T}x - \mathcal{T}y \rVert \leqslant \lVert x - y \rVert, ~~ \forall ~ x, y \in \dom{\mathcal{T}} := \left\{ w \in \R^n ~ \middle| ~ \mathcal{T} w \neq \emptyset \right\}.
\end{equation*}
\item[(2)] $0\in \mathcal{A}(x) + \mathcal{B}(x) \Longleftrightarrow \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}(z) = z, \ x = \mathcal{R}_{s\mathcal{B}}(z).$
\end{itemize}
\end{prop}

\begin{proof}
(1). 任取$(x, u), (y, v) \in \mathcal{R}_{s\mathcal{A}} = (\mathcal{I} + s \mathcal{A})^{-1},$ 那么有
\begin{equation*}
x \in u + s\mathcal{A}(u), ~~ y \in v + s\mathcal{A}(v).
\end{equation*}
由于$\mathcal{A}$是单调算子，因此 (由单调算子定义式~\eqref{eq:mo}) 有
\begin{equation*}
\frac{1}{s}((x - u) - (y - v))^T (u - v) \geqslant 0.
\end{equation*}
那么我们有
\begin{align*}
\lVert (2u - x) - (2v - y) \rVert^2 & = \lVert x - y \rVert^2 - 4 ((x - u) - (y - v))^T (u - v) \\
& \leqslant \lVert x - y \rVert^2,
\end{align*}
因此$\mathcal{C}_{s\mathcal{A}} = 2 \mathcal{R}_{s\mathcal{A}} - \mathcal{I}$是非扩张的算子。同理可证$\mathcal{C}_{s\mathcal{B}}$是非扩张的，从而$\mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}$也是非扩张算子。

(2). 我们有如下的等价关系\cite{ryu2022large}
\begin{align*}
0 \in \mathcal{A}(x) + \mathcal{B}(x) & \Longleftrightarrow 0 \in (\mathcal{I} + s\mathcal{A})(x) - (\mathcal{I} - s\mathcal{B})(x) \\
& \Longleftrightarrow 0 \in (\mathcal{I} + s\mathcal{A})(x) - (2\mathcal{I} - (\mathcal{I} + s\mathcal{B}))(x) \\
& \Longleftrightarrow 0 \in (\mathcal{I} + s\mathcal{A})(x) - \mathcal{C}_{s\mathcal{B}} (\mathcal{I} + s\mathcal{B})(x) \\
& \Longleftrightarrow 0 \in (\mathcal{I} + s\mathcal{A})(x) - \mathcal{C}_{s\mathcal{B}} z, ~ z \in (\mathcal{I} + s\mathcal{B}) (x) \\
& \Longleftrightarrow \mathcal{C}_{s\mathcal{B}} (z) \in (\mathcal{I} + s\mathcal{A}) \mathcal{R}_{s\mathcal{B}} (z) , ~ x \in \mathcal{R}_{s\mathcal{B}} (z) \\
& \Longleftrightarrow \mathcal{R}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}} (z) = \mathcal{R}_{s\mathcal{B}} (z), ~ x \in \mathcal{R}_{s\mathcal{B}} (z) \\
& \Longleftrightarrow 2\mathcal{R}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}} (z) - \mathcal{C}_{s\mathcal{B}} (z) = 2 \mathcal{R}_{s\mathcal{B}} (z) - z + z, ~ x \in \mathcal{R}_{s\mathcal{B}} (z) \\
& \Longleftrightarrow \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}} (z) = z, ~ x \in \mathcal{R}_{s\mathcal{B}} (z)
\end{align*}
\end{proof}

\begin{rem}
\label{rem:resolvent}
当$s$明确取定的时候，我们可以把算子$\mathcal{R}_{s\mathcal{A}}, \mathcal{R}_{s\mathcal{B}}, \mathcal{C}_{s\mathcal{A}}, \mathcal{C}_{s\mathcal{B}}$分别简写为$\mathcal{R}_{\mathcal{A}}, \mathcal{R}_{\mathcal{B}}, \mathcal{C}_{\mathcal{A}}, \mathcal{C}_{\mathcal{B}}.$ 这个命题告诉了我们，求解单调包含问题$0 \in \mathcal{A}(x) + \mathcal{B}(x),$等价于求解非扩张算子$\mathcal{C}_{\mathcal{A}} \mathcal{C}_{\mathcal{B}}$的不动点问题。命题~\ref{prop:os}~所涉及到的算子$\mathcal{R}_{\mathcal{A}}, \mathcal{R}_{\mathcal{B}}$被称作算子$s\mathcal{A}, s\mathcal{B}$的预解式 (Resolvent)\index{预解式, Resolvent}，即算子$s\mathcal{A}, s\mathcal{B}$的逆算子；$\mathcal{C}_{\mathcal{A}}, \mathcal{C}_{\mathcal{B}}$被称作反射预解式 (Reflected Resolvent)\index{反射预解式, Reflected Resolvent}，或者Cayley算子\index{Cayley算子}。特别地，当算子$\mathcal{A}$是某个函数$f$的次梯度 (或者梯度) 的时候，有
\begin{equation}
\label{eq:resolvent-prox-relation}
\mathcal{R}_{s\partial f} = (1 + s\partial f)^{-1} = \prox_{sf}.
\end{equation}
关于预解式相关的基础知识以及重要结果，读者可参阅文献\parencite{ryu2022large}的\S~2.5。
\end{rem}

这样，我们就把我们需要求解的单调包含问题~\eqref{eq:fedsplit-mono-incl}~转化为了以下的不动点问题
\begin{equation}
\label{eq:fixed-pt-pr}
\mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}} (z) = z, ~ x \in \mathcal{R}_{s\mathcal{B}} (z).
\end{equation}
我们称这种转化为Peaceman–Rachford分裂 (Peaceman–Rachford Splitting, PRS)\cite{Peaceman_1955_prsm,Lions_1979_splitting}\index{Peaceman–Rachford分裂, Peaceman–Rachford Splitting, PRS}。由于算子$\mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}$只是非扩张的，我们可以用$\tau$-平均算子$(1 - \tau) \mathcal{I} + \tau \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}$代替算子$\mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}},$ 其中$\tau \in (0, 1),$ 进而考虑如下的不动点问题
\begin{equation}
\label{eq:fixed-pt-general}
((1 - \tau) \mathcal{I} + \tau \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}) (z) = z, ~ x \in \mathcal{R}_{s\mathcal{B}} (z).
\end{equation}
特别地，当我们取$\tau = \frac{1}{2}$时，对应的不动点问题为
\begin{equation}
\label{eq:fixed-pt-dr}
\frac{1}{2}(\mathcal{I} + \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}}) (z) = z, ~ x \in \mathcal{R}_{s\mathcal{B}} (z),
\end{equation}
称这种转化为Douglas-Rachford分裂 (Douglas-Rachford Splitting, DRS)\cite{Douglas_1956_drsm,Lions_1979_splitting}\index{Douglas-Rachford分裂, Douglas-Rachford Splitting, DRS}。

求解不动点问题~\eqref{eq:fixed-pt-pr}, \eqref{eq:fixed-pt-general}, \eqref{eq:fixed-pt-dr}~的一般方法是进行不动点迭代 (Fixed Point Iteration, FPI)\index{不动点迭代, Fixed Point Iteration}
\begin{equation}
\label{eq:fixed-pt-iter}
z^{(t+1)} = \mathcal{T}(z^{(t)}), ~~ \text{$\mathcal{T}$是某个非扩张算子。}
\end{equation}
具体到$\mathcal{T} = (1 - \tau) \mathcal{I} + \tau \mathcal{C}_{s\mathcal{A}} \mathcal{C}_{s\mathcal{B}},$ $\tau \in (0, 1],$ 当$\tau = 1$时，我们有Peaceman–Rachford分裂方法 (Peaceman–Rachford Splitting Method, PRSM)\index{Peaceman–Rachford分裂方法, Peaceman–Rachford Splitting Method, PRSM}
\begin{subequations}
\label{eq:prsm}
\begin{align}
x^{(t+1/2)} & = \mathcal{R}_{\mathcal{B}}(z^{(t)}) \label{eq:prsm-1} \\
z^{(t+1/2)} & = 2x^{(t+1/2)} - z^{(t)} \label{eq:prsm-2} \\
x^{(t+1)} & = \mathcal{R}_{\mathcal{A}}(z^{(t+1/2)}) \label{eq:prsm-3} \\
z^{(t+1)} & = z^{(t)} + 2x^{(t+1)} - 2x^{(t+1/2)} \label{eq:prsm-4}
\end{align}
\end{subequations}
当$\tau = \frac{1}{2}$时，有Douglas–Rachford分裂方法 (Douglas–Rachford Splitting Method, DRSM)\index{Douglas–Rachford分裂方法, Douglas–Rachford Splitting Method, DRSM}
\begin{subequations}
\label{eq:drsm}
\begin{align}
x^{(t+1/2)} & = \mathcal{R}_{\mathcal{B}}(z^{(t)}) \label{eq:drsm-1} \\
z^{(t+1/2)} & = 2x^{(t+1/2)} - z^{(t)} \label{eq:drsm-2} \\
x^{(t+1)} & = \mathcal{R}_{\mathcal{A}}(z^{(t+1/2)}) \label{eq:drsm-3} \\
z^{(t+1)} & = z^{(t)} + x^{(t+1)} - x^{(t+1/2)} \label{eq:drsm-4}
\end{align}
\end{subequations}
我们注意到PRSM与DRSM的区别主要在迭代格式的最后一步~\eqref{eq:prsm-4}~与~\eqref{eq:drsm-4}，这对应的正是不动点迭代方法~\eqref{eq:fixed-pt-iter}~中算子$\mathcal{T}$的不同的选取。对于一般的$\tau \in (0, 1),$ 这一步的迭代为
\begin{equation}
\label{eq:sm-general-step}
z^{(t+1)} = z^{(t)} + 2\tau (x^{(t+1)} - x^{(t+1/2)}).
\end{equation}

我们现在回到我们想要求解的单调包含问题~\eqref{eq:fedsplit-mono-incl}，并将命题~\ref{prop:os}~应用到这个问题上。我们令
\begin{equation}
\label{eq:fedsplit-operator}
\mathcal{T} = \nabla F + \partial \iota_{\mathcal{E}} = \nabla F + \mathcal{N}_{\mathcal{E}}
\end{equation}
那么根据命题~\ref{prop:os}，我们将问题转化为了求算子$\mathcal{C}_{\mathcal{A}} \mathcal{C}_{\mathcal{B}}$的不动点的问题，其中$\mathcal{A} = \nabla F, \mathcal{B} = \partial \iota_{\mathcal{E}} = \mathcal{N}_{\mathcal{E}}.$ 对于这两个算子，根据~\eqref{eq:resolvent-prox-relation}~有
\begin{equation*}
\mathcal{R}_{\nabla F} = \prox_{F, \mu}, \quad \mathcal{R}_{\mathcal{N}_{\mathcal{E}}} = \prox_{\partial \iota_{\mathcal{E}}, \mu} = \Pi_{\mathcal{E}},
\end{equation*}
其中$s = \frac{1}{\mu},$ 同时对于分块可分离的目标函数，其临近算子是可分块计算的，即有
\begin{align*}
\prox_{F, \mu}(\Theta) & = \argmin_{\omega} \left\{ F(\omega) + \frac{\mu}{2} \lVert \omega - \Theta \rVert^2 \right\} \\
& = \argmin_{\omega} \left\{ \sum_{k=1}^K f_k(\omega_k) + \frac{\mu}{2} \sum_{k=1}^K \lVert \omega_k - \theta_k \rVert^2 \right\} \\
& = (\prox_{f_k, \mu}(\theta_k))_{j=1}^K.
\end{align*}
这意味着，我们可以为问题~\eqref{eq:fedsplit-pen}~的求解设计充分解耦合，从而适用于联邦学习场景的分裂算法。基于以上的问题转换，文献\parencite{pathak2020fedsplit}将PRSM应用到了联邦学习关注的问题~\eqref{eq:fedsplit-pen}，提出了联邦分裂算法\texttt{FedSplit}\index{联邦分裂算法, \texttt{FedSplit}}。这是分裂算法第一次应用于联邦学习领域。该算法的伪代码见算法~\ref{algo:fedsplit}，其中的$\texttt{prox\_update}_k$是求解子节点$k$上的临近算子$\prox_{f_k, \mu}$的非精确的方法，例如随机梯度下降。

\input{algorithms/fedsplit.tex}

联邦分裂算法\texttt{FedSplit}很好地解决了在本节开头的例~\ref{eg:correctness}~以及定理~\ref{thm:fedsplit-correctness}~中提出的关于联邦学习算法普遍存在的收敛正确性相关的问题，即我们有如下的结论

\begin{theorem}
\label{thm:fedsplit-main}
设$\Theta^* = (\theta_1^*, \ldots, \theta_K^*)$是联邦分裂算法\texttt{FedSplit}~\ref{algo:fedsplit}~迭代得到的不动点，即
\begin{equation*}
\theta_k^* = \theta_k^* + 2(\prox_{f_k, \mu}(2 \theta^* - \theta_k^*) - \theta^*), ~~ \forall ~ k \in [K],
\end{equation*}
其中$\theta^* = \frac{1}{K} \sum\limits_{k=1}^K \theta_k^*$为平均所有子节点模型参数得到的最终的全局模型参数，那么$\theta^*$是我们最初始要求解的优化问题~\eqref{eq:fl-basic}的最优解，即
\begin{equation*}
\sum\limits_{k=1}^K f_k(\theta^*) = \min\limits_{\theta \in \R^d} \sum\limits_{k=1}^K f_k(\theta).
\end{equation*}
\end{theorem}

% TODO: 添加证明

联邦分裂算法\texttt{FedSplit}有它自身的一些局限性。因为对于一般的只有非扩张性质的Cayley算子，PRSM~\eqref{eq:prsm}~并没有收敛性上的保证。因此，为了得到算法的收敛性，需要对目标函数$f_k$做一些额外的假设，例如强凸性，$L$-光滑性等。对于联邦分裂算法\texttt{FedSplit}在这些条件下的收敛率，有如下的结果

\begin{theorem}
\label{thm:fedsplit-conv-rate}
设$f_k$是$\ell_j$-强凸而且是$L_k$-光滑的函数, 令$\ell_* = \min \ell_k$, $L^* = \max L_k$, $\kappa = L^*/\ell_*.$ 取步长$s = 1 / \sqrt{\ell_*L^*} = \frac{1}{\mu},$ 并假设非精确求解方法$\texttt{prox\_update}_k$的误差界为$b > 0,$ 即有
\begin{equation*}
\lVert \texttt{prox\_update}_k(\theta) - \prox_{f_k, \mu}(\theta) \rVert \leqslant b, ~~ \forall k \in [K], ~ \theta \in \R^d,
\end{equation*}
那么
\begin{equation}
\label{eq:fedsplit-conv-rate}
\lVert \theta^{(t+1)} - \theta^* \rVert \leqslant \left( 1 - \frac{2}{\sqrt{\kappa}+1} \right)^t \frac{\lVert \theta^{(1)} - \theta^* \rVert}{\sqrt{K}} + (\sqrt{\kappa}+1)b.
\end{equation}
\end{theorem}

文献\parencite{tran2021feddr}基于文献\parencite{pathak2020fedsplit}引入的联邦分裂算法\texttt{FedSplit}存在的一些问题进行了改进，进一步将有收敛性保证的DRSM~\eqref{eq:drsm}~引入到联邦学习问题~\eqref{eq:fl-basic}~的研究，提出了联邦DR分裂算法\texttt{FedDR}\index{联邦DR分裂算法, \texttt{FedDR}}，将可处理问题的范围扩大到了目标函数$f_k$非凸的情形。事实上，相较于联邦学习的基本模型~\eqref{eq:fl-basic}，文献\parencite{tran2021feddr}进一步显式的考虑了正则项，相应的模型为
\begin{equation}
\label{eq:feddr-model}
\text{minimize} \quad F(\theta) = f(\theta) + g(\theta) = \frac{1}{K} \sum_{k=1}^K f_k(\theta) + g(\theta),
\end{equation}
其中$f_k$是$L$-光滑的非凸函数，$g$是正则项，一般假设是一个闭的真凸函数 (Closed Convex and Proper, CCP)\index{闭的真凸函数, Closed Convex and Proper, CCP}。类似~\eqref{eq:fedsplit-pen}, 我们将问题~\eqref{eq:feddr-model}~等价转换为如下的约束优化问题
\begin{equation}
\label{eq:feddr-constraint}
\begin{array}{cl}
\minimize & F(\Theta) := f(\Theta) + g(\Theta) = \frac{1}{K} \sum\limits_{k=1}^K f_k(\theta_k) + g(\theta_{K+1}), \\
\text{subject to} & \theta_1 = \cdots = \theta_{K+1} \in \R^d, ~ \Theta = \col(\theta_1, \cdots, \theta_{K+1}).
\end{array}
\end{equation}
并进一步通过罚函数$\iota_{\mathcal{E}},$ 将问题改写为无约束的问题
\begin{equation}
\label{eq:feddr-pen}
\minimize \quad f(\Theta) + g(\Theta) + \iota_{\mathcal{E}}(\Theta) = \frac{1}{K} \sum\limits_{k=1}^K f_k(\theta_k) + g(\theta_{K+1}) + \iota_{\mathcal{E}}(\Theta).
\end{equation}
需要注意的是，在式~\eqref{eq:feddr-constraint}~以及式~\eqref{eq:feddr-pen}~中为了记号简便，我们对自变量为$\Theta = \col(\theta_1, \cdots, \theta_{K+1})$的函数$\frac{1}{K} \sum\limits_{k=1}^K f_k(\theta_k)$以及以$\theta$为自变量的函数$\frac{1}{K} \sum\limits_{k=1}^K f_k(\theta)$使用了同一记号$f$ (以及$F$与$g$)。 问题~\eqref{eq:feddr-pen}~稳定点的一阶 (必要) 条件为
\begin{equation}
\label{eq:feddr-mono-incl}
0 \in \nabla f(\Theta) + \partial (g + \iota_\mathcal{E})(\Theta).
\end{equation}
如果令$\mathcal{B} = \nabla f,$ $\mathcal{A} = \partial (g + \iota_\mathcal{E}),$ 那么相应的预解式为
\begin{equation*}
\mathcal{R}_{\mathcal{B}} = \prox_{Ksf}, ~~ \mathcal{R}_{\mathcal{A}} = \prox_{Ks(g + \iota_{\mathcal{E}})}.
\end{equation*}
将DRSM~\eqref{eq:drsm} (更准确来说是其一般形式~\eqref{eq:sm-general-step}) 应用到这个问题，其迭代格式为
\begin{equation*}
\begin{aligned}
\omega_k^{(t+1)} & = \omega_k^{(t)} + \alpha (\overline{\theta}^{(t)} - \theta_k^{(t)}), ~ k \in [K] \\
\theta_k^{(t+1)} & = \prox_{f_k, \mu} (\omega_k^{(t+1)}), ~ k \in [K] \\
\widehat{\theta}_k^{(t+1)} & = 2\theta_k^{(t+1)} - \omega_k^{(t+1)}, ~ k \in [K] \\
\widetilde{\theta}^{(t+1)} & = \frac{1}{K}\sum\limits_{k=1}^K \widehat{\theta}_k^{(t+1)} \\
\omega^{(t+1)} & =  \omega^{(t)} + \alpha (\overline{\theta}^{(t)} - \omega^{(t)}) \\
\overline{\theta}^{(t+1)} & = \prox_{g, \frac{K+1}{Ks}} \left( \frac{K}{K+1} \widetilde{\theta}^{(t+1)} + \frac{1}{K+1} \omega^{(t+1)} \right)
\end{aligned}
\end{equation*}
至此，我们可以把联邦DR分裂算法\texttt{FedDR}以伪代码的形式总结为算法~\ref{algo:feddr}。

\input{algorithms/feddr.tex}

关于联邦DR分裂算法\texttt{FedDR}的收敛性，有如下的结论\cite{tran2021feddr}

\begin{theorem}[\parencite{tran2021feddr} Appendix A.]
\label{thm:feddr-convergence}
假设目标函数$f_k$都是$L$-光滑的。令$\{ (\theta^{(t)}, \theta_k^{(t)}) \}$为联邦DR分裂算法\texttt{FedDR}~\ref{algo:feddr}~迭代生成的模型参数序列，
\begin{itemize}
\item[(1)] 中心服务器上的全局模型参数与子节点上的模型参数的差有界
\begin{equation*}
\lVert \overline{\theta}^{(t)} - \theta_k^{(t)} \rVert^2 \leqslant \frac{2(1+s^2L^2)}{\alpha} \left[ (1+\gamma_1) \lVert \theta_k^{(t+1)} - \theta_k^{(t)} \rVert^2 \right. 
\left. + \frac{2(1+\gamma_1)}{\gamma_1} \left( \lVert e_k^{(t+1)} \rVert^2 + \lVert e_k^{(t)} \rVert^2 \right) \right],
\end{equation*}
其中$\gamma_1$是任意一个大于0的实数。也就是说 (经过正则项修正的) 全局模型与局部模型的差被局部模型相邻迭代步的差以及临近算子非精确求解的误差界所控制。
\item[(2)] 令
\begin{equation*}
\mathcal{G}_{\beta}(\theta) := \frac{1}{\beta} (x - \prox_{\beta g}(\theta - \beta \nabla f(\theta)))
\end{equation*}
为问题~\eqref{eq:feddr-model}~稳定点一阶条件$0 \in \nabla f(\theta) + \partial g(\theta)$对应的梯度映射，$\beta > 0,$ 那么
\begin{multline*}
\lVert \mathcal{G}_{\frac{ns}{n+1}} (\overline{\theta}^{(t)}) \rVert^2 \leqslant \frac{n+1}{n^2s^2} \left\{ (1+sL)^2 \sum_{k=1}^K (1+\gamma_2) \left[ \lVert \overline{\theta}^{(t)} - \theta_k^{(t)} \rVert^2 \right. \right. \\
\left. \left. + \frac{1+\gamma_2}{\gamma_2} \lVert e_k^{(t)} \rVert^2 \right] + \lVert \omega^{(t)} - \overline{\theta}^{(t)} \rVert^2 \right\}.
\end{multline*}
其中$\gamma_2$是任意一个大于0的实数。也就是说整体的目标函数在第$t$步迭代得到的 (经过正则项修正的) 全局模型参数$\overline{\theta}^{(t)}$处的梯度映射的值的大小 (模长平方) 被该步的全局模型与局部模型的差、临近算子非精确求解的误差界以及中间变量$\omega^{(t)}$与该点$\overline{\theta}^{(t)}$的差所控制。
\item[(3)] 存在常数$C_1, C_2, C_3,$ 使得下式成立
\begin{equation*}
\dfrac{1}{T} \sum_{t=0}^{T-1} \expectation \left[ \lVert \mathcal{G}_{\frac{Ks}{K+1}} (\overline{\theta}^{(t)}) \rVert^2 \right] \leqslant \frac{C_1(F(\theta^{(0)} - F^*)}{T} + \frac{1}{KT} \sum_{t=0}^{T-1} \sum_{k=1}^K (C_2 \varepsilon_{k,t}^2 + C_3 \varepsilon_{k,t+1}^2)
\end{equation*}
其中$F^* = \inf\limits_{\theta \in \R^d} F(\theta).$ 也就是说，令$M \geqslant \frac{1}{K} \sum_{t=0}^{T-1} \sum_{k=1}^K \varepsilon_{k,t}^2,$ 联邦DR分裂算法\texttt{FedDR}~\ref{algo:feddr}~迭代$T$步之后，其中
\begin{equation*}
T = \left\lfloor \frac{C_1 [F(\theta^{(0)}) - F^*] + (C_2 + C_2)M}{\varepsilon^2} \right\rfloor,
\end{equation*}
得到的全局模型参数$\overline{\theta}^{(T)}$即为原问题~\eqref{eq:feddr-model}的$\varepsilon$-稳定点，即
\begin{equation*}
\expectation [ \lVert \mathcal{G}_{\beta}(\overline{\theta}^{(T)}) \rVert^2 ] < \varepsilon^2.
\end{equation*}
\end{itemize}
\end{theorem}

算子分裂法是分解算法 (Decomposition Algorithms)\index{分解算法, Decomposition Algorithms} 的一大类，分解算法基本思想是将大规模的问题分解成一系列小规模的问题，将每个子问题分开处理，使得迭代算法中的子问题更容易求解或者更容易并行化。PRSM以及DRSM都是将要求解的问题\ref{eq:fl-basic}分为了两部分 (见单调包含问题~\eqref{eq:fedsplit-mono-incl}~以及~\eqref{eq:feddr-mono-incl})，进行交替迭代求解。

分解算法在理论分析与数值效果上都有着独特的优势。通过定理~\ref{thm:fedsplit-correctness}以及定理~\ref{thm:feddr-convergence}，我们可以看到分裂算法的确能够收敛到我们想要求解问题的极值点或者稳定点，这相较于联邦平均算法\texttt{FedAvg}~\ref{algo:fedavg}，联邦邻近算法\texttt{FedProx}~\ref{algo:fedprox}~是极大的进步。其代价，或者说缺点，就是除了模型参数本身，每一个计算节点 (包括中心服务器与子节点) 还有一些额外的辅助参数需要维护，增加了相关的存储以及运算的负担。
