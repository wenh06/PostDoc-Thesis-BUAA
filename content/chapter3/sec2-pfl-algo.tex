\section{个性化联邦学习中的典型算法}
\addcontentsline{toe}{section}{{3.2\ \ Existing Algorithms for Personalized Federated Learning}\numberline\,}
\label{sec:chap3-pfl-algo}

% NOT finished
% NOT indexed

本节主要介绍已有的一些基于正则化目标函数的个性化联邦学习算法，此类算法的基本形式见上一节\S~\ref{sec:chap3-pfl}~的式~\eqref{eq:pfl-general}。本文称这一类的基于正则化目标函数的最优化问题为弱共识问题 (Weak Consensus Problem)\index{弱共识问题, Weak Consensus Problem}。本节将介绍一些基于此类问题而提出的个性化联邦学习算法。

\subsection*{无环随机梯度下降算法\texttt{L2SGD}}
% almost finished

这一类算法形式最简单的是文献\cite{hanzely2020federated}所引入的无环随机梯度下降 (Loopless Local SGD, L2SGD)\index{无环随机梯度下降, Loopless Local SGD, L2SGD} 算法\texttt{L2SGD}。其考虑的优化问题格式为
\begin{equation}
\label{eq:l2sgd}
\begin{array}{cl}
\minimize & F(\Theta) = f(\Theta) + \lambda \varphi(\Theta) \\
\text{where} & f(\Theta) = \frac{1}{K} \sum\limits_{k=1}^K f_i(\theta_i) \\
& \varphi(\Theta) = \frac{1}{2K} \sum\limits_{k=1}^K \left\lVert \theta_k - \bar{\theta} \right\rVert^2 = \frac{1}{2K} \sum\limits_{k=1}^K \left\lVert \theta_k - \frac{1}{K} \sum\limits_{j=1}^K \theta_j \right\rVert^2 \\
& \Theta = \col(\theta_1, \ldots, \theta_K).
\end{array}
\end{equation}
其中$\lambda \geqslant 0$是罚参数。当$0 < \lambda < \infty,$ 求解上述优化问题得到的模型$\bar{\theta}, \theta_1, \ldots, \theta_K$被称作是混合模型 (Mixed Models)。很容易看出，上述问题可以等价地转化为联邦临近算法\texttt{FedProx}所考察的问题~\eqref{eq:fedprox-whole}。

\input{algorithms/l2sgd}

文献\parencite{hanzely2020federated}定义问题~\ref{eq:l2sgd}目标函数$F(\Theta)$的随机梯度为
\begin{equation}
\label{eq:l2sgd-grad}
G(\Theta) := \begin{cases}
\frac{\nabla f(\Theta)}{1 - p}, & \text{概率$1-p$} \\
\frac{\lambda \nabla \varphi(\Theta)}{p}, & \text{概率$p$}
\end{cases}
\end{equation}
以上定义的随机梯度是是梯度$\nabla F$的无偏估计。这样的方法简化了$\nabla F$的计算，提高了整体算法的收敛率\cite{Kovalev2020_loopless}。值得特别注意的是，在随机梯度$G$的定义式~\eqref{eq:l2sgd-grad}~中，$\nabla f(\Theta) = \col(\nabla f_1(\theta_1), \cdots, \nabla f_K(\theta_K))$的计算是完全块分离的，在联邦学习的场景下，这一计算步骤只需要在子节点上执行计算，并不涉及与中心节点的通信，这对于以通信为主要瓶颈的联邦学习场景是巨大的优势。从这个意义上来说，无环随机梯度下降\texttt{L2SGD}也融入了跳步更新的思想。

文献\parencite{hanzely2020federated}对子节点上的目标函数做了进一步的假设，即假设$f_k$有有限和的结构
\begin{equation}
f_k(\theta_k) = \sum\limits_{j=1}^m f_{k,j}(\theta_k),
\end{equation}
并结合随机Kaczmarz方法 (Randomized Kaczmarz Method)\index{随机Kaczmarz方法, Randomized Kaczmarz Method}\cite{Strohmer_2008_Kaczmarz,Needell_2015_Kaczmarz} 或者说结合基于采样的随机梯度下降法 (SGD Algorithm with Importance Sampling)\cite{Needell_2015_Kaczmarz,Zhao2015_sampling}，为问题~\eqref{eq:l2sgd}~的求解设计了所谓的无环随机梯度下降法，其伪代码见算法~\ref{algo:l2sgd}。实际上，随机梯度~\eqref{eq:l2sgd-grad}~也是基于采样的随机梯度。基于采样的随机梯度算法从收敛率到通信效率都有提升，是值得进一步研究的联邦学习优化算法。

\subsection*{\texttt{pFedMe}算法}
% almost finished

文献\parencite{t2020pfedme}考虑了与~\eqref{eq:l2sgd}~类似形式的问题
\begin{equation}
\label{eq:pfedme-one}
\minimize_{\theta,\theta_1,\ldots,\theta_K} \quad \sum\limits_{k=1}^K \left\{ f_k(\theta_k) + \frac{\lambda}{2} \left\lVert \theta_k - \theta \right\rVert^2 \right\}.
\end{equation}
特别地，文献\parencite{t2020pfedme}基于联邦学习中心节点--子节点这一二元结构，进一步将上述问题等价地转换为一个双层优化问题 (Bi-level Optimization Problem)\index{双层优化问题, Bi-level Optimization Problem}
\begin{equation}
\label{eq:pfedme-bilevel}
\begin{array}{cl}
\minimize & F(\theta) := \frac{1}{K} \sum\limits_{k=1}^K F_k(\theta), \\
\text{where} & F_k(\theta) = \min\limits_{\theta_k \in \R^d} \left\{ f_k(\theta_k) + \frac{\lambda}{2} \left\lVert \theta_k - \theta \right\rVert^2 \right\},
\end{array}
\end{equation}
相应求解算法\texttt{pFedMe}的伪代码见算法~\ref{algo:pfedme}。

\input{algorithms/pfedme}

很容易看到问题~\eqref{eq:pfedme-bilevel}~中的$F_k(\theta)$即为$f_k(\theta_k)$的Moreau包络~\eqref{eq:moreau_env}
\begin{equation*}
F_k(\theta) = \mathcal{M}_{f_k, \lambda} (\theta) := \inf\limits_{\theta_k} \left\{ f_k(\theta_k) + \frac{\lambda}{2} \lVert \theta_k - \theta \rVert^2 \right\}
\end{equation*}
相应最优化问题的解点即为临近算子
\begin{equation*}
\prox_{f_k, \lambda}: \theta \mapsto \argmin_{\theta_k} \left\{ f_k(\theta_k) + \frac{\lambda}{2} \lVert \theta_k - \theta \rVert^2 \right\}
\end{equation*}
在全局模型$\theta$处的取值。这个值即为算法\texttt{pFedMe}得到的子节点$k$上的个性化模型。算法\texttt{pFedMe}对双层优化~\eqref{eq:pfedme-bilevel}~的外层问题，即$F(\theta)$的优化求解采取的是随机梯度下降法。通过Moreau包络与临近算子的关系式~\eqref{eq:prox-moreau-relation}~可知双层优化问题~\eqref{eq:pfedme-bilevel}~的外层目标函数$F(\theta)$的梯度可以通过下式计算
\begin{equation}
\label{eq:pfedme-grad}
\nabla F(\theta) = \frac{1}{K} \sum\limits_{k=1}^K \nabla F_k(\theta) = \frac{1}{K} \sum\limits_{k=1}^K \nabla \mathcal{M}_{f_k, \lambda} (\theta) = \frac{1}{K} \sum\limits_{k=1}^K \lambda \left( \theta - \prox_{f_k, \lambda} (\theta) \right),
\end{equation}
相应的梯度下降的迭代格式为
\begin{equation}
\label{eq:pfedme-iter}
\begin{aligned}
\theta^{(t+1)} & = \theta^{(t)} - \eta \nabla F(\theta^{(t)}) = \theta^{(t)} - \eta \lambda \frac{1}{K} \sum\limits_{k=1}^K \left( \theta^{(t)} - \prox_{f_k, \lambda} (\theta^{(t)}) \right) \\
& = \frac{1}{K} \sum\limits_{k=1}^K \left( \theta^{(t)} - \eta \lambda \left( \theta^{(t)} - \prox_{f_k, \lambda} (\theta^{(t)}) \right) \right).
\end{aligned}
\end{equation}
以上格式可以分解为两部分，第一部分是在子节点上依次执行临近算子$\prox_{f_k, \lambda}$的求解
\begin{equation}
\label{eq:pfedme-prox-1}
\texttt{prox\_update}_k (\theta^{(t)}),
\end{equation}
以及全局模型的部分更新：
\begin{equation}
\label{eq:pfedme-local-1}
\theta^{(t)} - \eta \lambda \left( \theta^{(t)} - \prox_{f_k, \lambda} (\theta^{(t)}) \right).
\end{equation}
实际上，算法\texttt{pFedMe}的子节点上有全局模型$\theta^{(t)}$的一份拷贝$\omega_k^{(t,r)}$，这两个部分的更新实际上执行的是，利用全局模型的拷贝$\omega_k^{(t,r)}$更新个性化模型$\theta_k^{(t, r)}:$
\begin{equation}
\label{eq:pfedme-prox-2}
\theta_k^{(t, r)} \gets \texttt{prox\_update}_k (\omega_{k}^{(t, r)}; b_r)
\end{equation}
以及$\omega_k^{(t,r)}$自身的更新：
\begin{equation}
\label{eq:pfedme-local-2}
\omega_k^{(t,r+1)} \gets \omega_k^{(t,r)} - \eta \lambda \left( \omega_k^{(t,r)} - \prox_{f_k, \lambda} (\omega_k^{(t,r)}) \right).
\end{equation}
第二部分是子节点模型在中心节点上的聚合：
\begin{equation}
\label{eq:pfedme-avg}
\theta^{(t+1)} \gets (1-\beta)\theta^{(t)} + \frac{\beta}{\# \mathcal{S}^{(t)}} \sum\limits_{k \in \mathcal{S}^{(t)}} \omega_{k}^{(t, R)}.
\end{equation}
算法\texttt{pFedMe}整个迭代循环值得特别注意的是，以上分解的两部分并不是交替进行的，而是在子节点上执行一定步数($R$步)的第一部分的迭代之后，才进行与中心节点的通信，在中心节点上执行第二部分的聚合。这是另一种形式的跳步更新的方法。

\subsection*{\texttt{Ditto}算法}
% almost finished

文献\parencite{li_2021_ditto}进一步发展了联邦临近算法\texttt{FedProx}\cite{sahu2018fedprox}添加临近项求解子问题的思想，将子节点的优化问题~\eqref{eq:fedprox}~改进为一个双层优化问题
\begin{equation}
\label{eq:ditto-bilevel}
\begin{array}{cl}
\minimize\limits_{\omega_k} & h_k(\omega_k, \theta^*) := f_k(\omega_k) + \frac{\mu}{2} \lVert \omega_k - \theta^* \rVert^2, \\
\text{subject to} & \theta^* \in \argmin\limits_{\theta} G(f_1(\theta)， \cdots, f_K(\theta)),
\end{array}
\end{equation}
其中$G$是中心节点上执行子节点模型参数聚合的函数，例如\texttt{FedAvg}算法中采用的求均值函数。这一问题格式是以子节点为主视角的格式，是第一批\footnote{稍早提出的自适应个性化联邦学习算法\texttt{APFL}\cite{deng2020_apfl}~\ref{algo:apfl}~也采用了类似的视角}明确基于这一视角的个性化联邦学习建模、求解方法。基于这一问题格式，文献\parencite{li_2021_ditto}设计了\texttt{Ditto}算法进行优化求解，其伪代码见算法~\ref{algo:ditto}。

\input{algorithms/ditto}

可以看到，文献\parencite{li_2021_ditto}考虑的双层优化问题~\eqref{eq:ditto-bilevel}~的内层问题实际上可以视作一个独立的联邦优化问题，只对全局模型参数进行优化求解；外层问题实际上只涉及了子节点上求解临近算子的问题。如果以中心节点为主视角，那么问题~\eqref{eq:ditto-bilevel}~实际上就是联邦平均算法\texttt{FedAvg}考虑的问题~\eqref{eq:fedavg-constraint}，同时在每个子节点上附带一个求解临近算子的问题达到模型个性化的目的。这个个性化的模型并不参与全局模型的更新。这样，全局模型与个性化模型被解耦合，迭代更新可以更灵活地进行。

类似于\texttt{pFedMe}算法~\ref{algo:pfedme}，\texttt{Ditto}算法~\ref{algo:ditto}~的子节点实际要执行2个优化问题的求解，即全局模型的更新 (双层优化问题~\eqref{eq:ditto-bilevel}~内层问题的求解)
\begin{equation}
\label{eq:ditto-global}
\theta_k^{(t)} \gets \operatorname{\mathbf{UpdateGlobal}}(\theta^{(t)}, f_k),
\end{equation}
以及临近算子的求解 (双层优化问题~\eqref{eq:ditto-bilevel}~外层问题的求解)
\begin{equation}
\label{eq:ditto-local-1}
\omega_k^{(t+1)} \gets \prox_{f_k, \mu} (\theta^{(t)}) = \argmin\limits_{\omega_k} \left\{ f_k(\omega_k) + \frac{\mu}{2} \lVert \omega_k - \theta^{(t)} \rVert^2 \right\}.
\end{equation}
与本文之前介绍的算法 (\texttt{FedSplit}算法~\ref{algo:fedsplit}、\texttt{pFedMe}算法~\ref{algo:pfedme}~等) 不同之处在于，\texttt{Ditto}算法在每次循环对以上问题的求解只执行一步 (随机) 梯度下降
\begin{equation}
\label{eq:ditto-local-2}
\omega_k^{(t+1)} \gets \omega_k^{(t)} - \eta \left( \nabla f_k(\omega_k^{(t)}) + \mu (\omega_k^{(t)} - \theta^{(t)}) \right),
\end{equation}
这样以伴随全局模型的更新而不断修正临近算子的中心项$\theta^{(t)},$ 而完成双层优化问题~\eqref{eq:ditto-bilevel}~外层问题的求解。这是自然的，因为这部分更新是双层优化问题的外层问题的迭代更新，而\texttt{FedSplit}算法、\texttt{pFedMe}算法相应的则是内循环以及内层问题的更新。

\subsection*{自适应个性化联邦学习算法\texttt{APFL}}
% almost finished

比\parencite{li_2021_ditto}稍早的文献\parencite{deng2020_apfl}也考虑了类似~\eqref{eq:ditto-bilevel}~的双层优化问题
\begin{equation}
\label{eq:apfl-bilevel}
\begin{array}{cl}
\minimize\limits_{\omega_k \in \dom f_k} & h_k(\omega_k, \theta^*) := f_k( \alpha_k \omega_k + (1 - \alpha_k) \theta^* ), \\
\text{subject to} & \theta^* \in \argmin\limits_{\theta} \frac{1}{K} \sum\limits_{k=1}^K (f_1(\theta) + \cdots + f_K(\theta)),
\end{array}
\end{equation}
其中$\alpha_k \in (0, 1)$被称作是混合权重 (Mixture Weights)。文献\parencite{deng2020_apfl}针对这一双层优化问题~\eqref{eq:apfl-bilevel}~基于随机梯度下降方法提出了自适应个性化联邦学习算法\texttt{APFL}，其伪代码可见算法~\ref{algo:apfl}。

\input{algorithms/apfl}

可以看到，与问题格式~\eqref{eq:ditto-bilevel}~最主要的区别是，\eqref{eq:apfl-bilevel}的外层问题对应的模型个性化的方法，采用了辅助模型参数$\omega_k$与全局模型$\theta^*$的凸组合$\alpha_k \omega_k + (1 - \alpha_k) \theta^*,$ 而不是临近项$\frac{\mu}{2} \lVert \omega_k - \theta^* \rVert^2$的的形式；而混合权重$\alpha_k$起到临近项中罚参数$\mu$类似的调节子节点上模型个性化程度的作用。最终得到的个性化模型为
\begin{equation}
\label{eq:apfl-per-model}
\begin{array}{cl}
& \hat{\omega}_k = \alpha_k \omega_k^* + (1 - \alpha_k) \theta^*, \\
\text{其中} & \omega_k^* = \argmin\limits_{\omega_k \in \dom f_k} f_k( \alpha_k \omega_k + (1 - \alpha_k) \theta^* ).
\end{array}
\end{equation}
值得特别注意的是，当$\dom f_k$是全空间$\R^d$的一个真子集时，通过式~\eqref{eq:apfl-per-model}个性化模型$\hat{\omega}_k$可能并非目标函数$f_k$的极小点。具体如图~\ref{fig:apfl}~所示，当取$\alpha_k = \frac{1}{2},$ 同时假设$\dom f_k,$ 即$\omega_k$的搜索区域是图中浅灰色阴影区域时，凸组合$\alpha_k \omega_k^* + (1 - \alpha_k) \theta^*$能取到的区域实际上是图中深灰色区域。可以看到深灰色区域并不包含$f_k$的极小点。当$\omega_k$的搜索区域变大，变成图~\ref{fig:apfl}~中的区域$\widetilde{\dom f_k}$时，通过迭代搜索得到的$\omega_k^*$才能使得$\hat{\omega}_k$成为$f_k$的极小点。但是在这种情况下，$\hat{\omega}_k$完全可以脱离整个联邦学习的训练网络，通过直接极小化$f_k$ (即只使用节点$k$上自身的数据进行训练) 获取，这与个性化联邦学习算法的目的是相违背的。

\input{tikz-figures/chap3-apfl}

\subsection*{动态正则化联邦学习算法\texttt{FedDyn}}
% NOT finished

\parencite{acar2021feddyn}  \texttt{FedDyn}待写。。。。

\input{algorithms/feddyn}


% \begin{equation}
% \label{eq:fedu}
% \minimize \quad \sum\limits_{k=1}^K f_k(x_k) + \dfrac{\lambda}{2} \sum\limits_{k=1}^K \sum\limits_{j\in\mathcal{N}_k} \lVert \theta_k - \theta_j \rVert^2
% \end{equation}


\subsection*{\texttt{pFedMac}算法}
% NOT finished

\parencite{li2021pfedmac}  \texttt{pFedMac}待写。。。。

\begin{equation}
\label{eq:pfedmac}
\begin{array}{cl}
\minimize & F(\theta) = \frac{1}{K} \sum\limits_{k=1}^K F_i(\theta), \\
\text{where} & F_k(\theta) := \min\limits_{\theta_k} \left\{ f_k(\theta_k) - \mu \langle \theta_k, \theta \rangle + \frac{\mu}{2} \lVert \theta \rVert^2_2 \right\}
\end{array}
\end{equation}

\begin{equation}
\label{eq:pfedmac-sparse}
\begin{array}{cl}
\minimize & F(\theta) = \frac{1}{K} \sum\limits_{k=1}^K F_i(\theta), \\
\text{where} & F_k(\theta) := \min\limits_{\theta_k} \left\{ f_k(\theta_k) - \mu \langle \theta_k, \theta \rangle + \frac{\mu}{2} \lVert \theta \rVert^2_2 + \lVert \theta_k \rVert_1 \right\}
\end{array}
\end{equation}

\input{algorithms/pfedmac}

待写。。。。
