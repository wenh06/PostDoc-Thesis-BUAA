\begin{algorithm}[ht]
% \SetAlgoNoLine
% \DontPrintSemicolon
\SetKwInOut{Input}{Input}
\Input{penalty coefficient $\lambda,$ learning rate $\eta,$ probability $p \in (0, 1)$}
{\bfseries Initiation:}\;
\Indp
    {\bfseries Clients init:} local model parameters $\theta_k^{(0)} \in \R^d, ~ \forall k \in [K]$\;
\Indm
\For{each round $t = 0, 1, \cdots, T-1$}{
    with probability $1-p$: \tcc*[h]{no global communication, only clients update}\;
    \Indp
    \For{each client $k \in [K]$ {\bfseries in parallel}}{
        sample $j \in [m]$ (uniformly at random)\;
        $g_k^{(t)} \gets \frac{1}{K(1-p)} \left( \nabla f_{k,j}(\theta_k^{(t)}) \right)$\;
        $\theta_k^{(t+1)} \gets \theta_k^{(t)} - \eta g_k^{(t)}$\;
    }
    \Indm
    with probability $p$:\;
    \Indp
    client $k$ send $\theta_{k}^{(t)}$ to server $\forall k \in [K]$\;
    {\bfseries Server Update:}\;
    \Indp
        $\theta^{(t)} \gets \frac{1}{K} \sum\limits_{k=1}^K \theta_{k}^{(t)}$ \tcc*[h]{compute global average}\;
    \Indm
    server broadcast $\theta^{(t)}$ to clients $k \in [K]$\;
    {\bfseries Clients Update:}\;
    \Indp
    \For{each client $k \in [K]$ {\bfseries in parallel}}{
        $g_k^{(t)} \gets \frac{\lambda}{Kp} \left( \theta_k^{(t)} - \theta^{(t)} \right)$\;
        $\theta_k^{(t+1)} \gets \theta_k^{(t)} - \eta g_k^{(t)}$\;
    }
    \Indm
}
\caption{算法\texttt{L2SGD}\cite{hanzely2020federated}的伪代码}
\label{algo:l2sgd}
\end{algorithm}
